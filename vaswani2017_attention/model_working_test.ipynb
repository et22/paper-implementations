{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# multihead self-attention implementation\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, use_enc_embed=False, use_mask=False, mask=None):\n",
    "        super().__init__()\n",
    "        self.encode = 0\n",
    "        self.h = h\n",
    "        self.W_O = nn.Parameter(torch.rand(h*d_v, d_model))\n",
    "        self.attn = nn.ModuleList([ScaledDotProductAttention(d_model, d_k, d_v, use_enc_embed=use_enc_embed, use_mask=use_mask, mask=mask) for _ in range(h)])\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def forward(self, x, enc_embed=None):\n",
    "        attn_out = torch.zeros((x.shape[0], x.shape[1], self.d_v*self.h))\n",
    "\n",
    "        for i in range(self.h):\n",
    "            attn_out[:, :, i*self.d_v:(i+1)*self.d_v] = self.attn[i](x, enc_embed)\n",
    "\n",
    "        return attn_out @ self.W_O\n",
    "    \n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, use_enc_embed=False, use_mask=False, mask=None):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.rand((d_model, d_k)))\n",
    "        self.W_K = nn.Parameter(torch.rand((d_model, d_k)))\n",
    "        self.W_V = nn.Parameter(torch.rand(d_model, d_v))\n",
    "        self.d_k = d_k\n",
    "        self.use_mask = use_mask\n",
    "        self.mask = mask\n",
    "        self.use_enc_embed = use_enc_embed\n",
    "\n",
    "    def forward(self, x, enc_embed):\n",
    "        if self.use_enc_embed: \n",
    "            v = torch.matmul(enc_embed, self.W_V)\n",
    "            k = torch.matmul(enc_embed, self.W_K)\n",
    "        else:\n",
    "            k = torch.matmul(x, self.W_K)\n",
    "            v = torch.matmul(x, self.W_V)\n",
    "        \n",
    "        q = torch.matmul(x, self.W_Q)\n",
    "\n",
    "        if self.use_mask:\n",
    "            mask = nn.Transformer.generate_square_subsequent_mask(q.shape[1])\n",
    "            weights = F.softmax(mask + torch.matmul(q, torch.permute(k, (0, 2, 1)))/torch.sqrt(torch.tensor(self.d_k)), dim=-1)\n",
    "            return weights @ v\n",
    "        else:\n",
    "            return F.softmax(torch.matmul(q, torch.permute(k, (0, 2, 1)))/torch.sqrt(torch.tensor(self.d_k)), dim=-1) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component testing against pytorch implementations\n",
    "# 1. test that our multihead attention implementation is correct (at least for encoder without masking)\n",
    "\n",
    "# test 1 - with 1 head \n",
    "pt_attn = nn.MultiheadAttention(embed_dim=512, num_heads=1, bias=False, batch_first = True) # pytorch implementation\n",
    "input = torch.rand((1, 8, 512))\n",
    "pt_attn_output, pt_attn_weights = pt_attn(input, input, input)\n",
    "\n",
    "my_attn = MultiHeadedAttention(512, 512, 512, 1, use_enc_embed=False, use_mask=False, mask=None) # my implementation\n",
    "my_attn.attn[0].W_Q = nn.Parameter(pt_attn.in_proj_weight[:512, :].T) # use same parameter values\n",
    "my_attn.attn[0].W_K = nn.Parameter(pt_attn.in_proj_weight[512:2*512, :].T)\n",
    "my_attn.attn[0].W_V = nn.Parameter(pt_attn.in_proj_weight[2*512:3*512, :].T)\n",
    "my_attn.W_O = nn.Parameter(pt_attn.out_proj.weight.T)\n",
    "my_attn_output = my_attn(input)\n",
    "\n",
    "assert(torch.allclose(my_attn_output, pt_attn_output, atol=1e-5, rtol=1e-5))\n",
    "\n",
    "# test 2 - with 2 heads \n",
    "pt_attn = nn.MultiheadAttention(embed_dim=512, num_heads=2, bias=False, batch_first = True) # pytorch implementation\n",
    "input = torch.rand((1, 8, 512))\n",
    "pt_attn_output, pt_attn_weights = pt_attn(input, input, input)\n",
    "\n",
    "my_attn = MultiHeadedAttention(512, 256, 256, 2, use_enc_embed=False, use_mask=False, mask=None) # my implementation\n",
    "\n",
    "my_attn.attn[0].W_Q = nn.Parameter(pt_attn.in_proj_weight[:256, :].T) # use same parameter values\n",
    "my_attn.attn[1].W_Q = nn.Parameter(pt_attn.in_proj_weight[256:2*256, :].T)\n",
    "\n",
    "my_attn.attn[0].W_K = nn.Parameter(pt_attn.in_proj_weight[2*256:3*256, :].T)\n",
    "my_attn.attn[1].W_K = nn.Parameter(pt_attn.in_proj_weight[3*256:4*256, :].T) # use same parameter values\n",
    "\n",
    "my_attn.attn[0].W_V = nn.Parameter(pt_attn.in_proj_weight[4*256:5*256, :].T)\n",
    "my_attn.attn[1].W_V = nn.Parameter(pt_attn.in_proj_weight[5*256:6*256, :].T)\n",
    "\n",
    "my_attn.W_O = nn.Parameter(pt_attn.out_proj.weight.T)\n",
    "my_attn_output = my_attn(input)\n",
    "\n",
    "assert(torch.allclose(my_attn_output, pt_attn_output, atol=1e-3, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoder implementation\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, input_len):\n",
    "        super().__init__()\n",
    "        self.input_len = input_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self):\n",
    "        pos = torch.zeros(self.d_model, self.input_len)\n",
    "\n",
    "        for i in range(int(self.input_len/2)):\n",
    "            pos[:, 2*i] = torch.sin(torch.range(start=0, end=self.d_model-1, step=1)/(10000 ** (2*i/self.input_len)))\n",
    "            pos[:, 2*i+1] = torch.cos(torch.range(start=1, end=self.d_model, step=1)/(10000 ** (2*i/self.input_len)))\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_60591/1624865870.py:12: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i] = torch.sin(torch.range(start=0, end=self.d_model-1, step=1)/(10000 ** (2*i/self.input_len)))\n",
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_60591/1624865870.py:13: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i+1] = torch.cos(torch.range(start=1, end=self.d_model, step=1)/(10000 ** (2*i/self.input_len)))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize positional encoding as sanity check \n",
    "# confirm that matches other peoples plots - e.g., https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "import matplotlib.pyplot as plt\n",
    "pos_enc = PositionalEncoder(50, 130)\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.imshow(pos_enc(), cmap='RdBu')\n",
    "plt.xlabel('encoding dimension')\n",
    "plt.ylabel('position index')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer encoder layer implementation\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k=4, d_v=4, h=4, d_ff = 128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.mha = MultiHeadedAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x + self.mha(x))\n",
    "        x = self.layer_norm(x + self.linear2(self.relu(self.linear1(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component testing against pytorch implementations\n",
    "pt_encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=1, dim_feedforward=2048, dropout=0, batch_first=True)\n",
    "input = torch.rand(1, 8, 512)\n",
    "pt_enc_out = pt_encoder_layer(input)\n",
    "\n",
    "my_encoder_layer = EncoderLayer(512, d_k=512, d_v=512, h=1, d_ff=2048) # my implementation\n",
    "\n",
    "# transfer multiheaded attention parameters to my layer\n",
    "my_encoder_layer.mha.attn[0].W_Q = nn.Parameter(pt_encoder_layer.self_attn.in_proj_weight[:512, :].T) # use same parameter values\n",
    "my_encoder_layer.mha.attn[0].W_K = nn.Parameter(pt_encoder_layer.self_attn.in_proj_weight[512:2*512, :].T)\n",
    "my_encoder_layer.mha.attn[0].W_V = nn.Parameter(pt_encoder_layer.self_attn.in_proj_weight[2*512:3*512, :].T)\n",
    "my_encoder_layer.mha.W_O = nn.Parameter(pt_encoder_layer.self_attn.out_proj.weight.T)\n",
    "\n",
    "# transfer linear layer parameters to my layer\n",
    "my_encoder_layer.linear1.weight = nn.Parameter(pt_encoder_layer.linear1.weight)\n",
    "my_encoder_layer.linear1.bias = nn.Parameter(pt_encoder_layer.linear1.bias)\n",
    "\n",
    "my_encoder_layer.linear2.weight = nn.Parameter(pt_encoder_layer.linear2.weight)\n",
    "my_encoder_layer.linear2.bias = nn.Parameter(pt_encoder_layer.linear2.bias)\n",
    "\n",
    "my_enc_out = my_encoder_layer(input)\n",
    "\n",
    "assert(torch.allclose(my_enc_out, pt_enc_out, atol=1e-3, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer decoder layer implementation\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k=4, d_v=4, h=4, d_ff = 128, use_mask=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.mmha = MultiHeadedAttention(d_model, d_k, d_v, h, use_mask=use_mask, mask=1)\n",
    "        self.mha = MultiHeadedAttention(d_model, d_k, d_v, h, use_enc_embed=True)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, target, memory):\n",
    "        x = self.layer_norm(target + self.mmha(target))\n",
    "        x = self.layer_norm(x + self.mha(x, memory))\n",
    "        x = self.layer_norm(x + self.linear2(self.relu(self.linear1(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component testing against pytorch implementations\n",
    "# test 1 - transformer decoder without causal masking\n",
    "pt_decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=1, dim_feedforward=2048, dropout=0, batch_first=True, bias=True)\n",
    "\n",
    "memory = torch.rand(1, 8, 512)\n",
    "target = torch.rand(1, 8, 512)\n",
    "pt_dec_out = pt_decoder_layer(target, memory) #, tgt_mask = nn.Transformer.generate_square_subsequent_mask(8), tgt_is_causal=True)\n",
    "\n",
    "my_decoder_layer = DecoderLayer(512, d_k=512, d_v=512, h=1, d_ff=2048) # my implementation\n",
    "\n",
    "# transfer masked multiheaded attention parameters to my layer\n",
    "my_decoder_layer.mmha.attn[0].W_Q = nn.Parameter(pt_decoder_layer.self_attn.in_proj_weight[:512, :].T) # use same parameter values\n",
    "my_decoder_layer.mmha.attn[0].W_K = nn.Parameter(pt_decoder_layer.self_attn.in_proj_weight[512:2*512, :].T)\n",
    "my_decoder_layer.mmha.attn[0].W_V = nn.Parameter(pt_decoder_layer.self_attn.in_proj_weight[2*512:3*512, :].T)\n",
    "my_decoder_layer.mmha.W_O = nn.Parameter(pt_decoder_layer.self_attn.out_proj.weight.T)\n",
    "\n",
    "# transfer multiheaded attention parameters to my layer\n",
    "my_decoder_layer.mha.attn[0].W_Q = nn.Parameter(pt_decoder_layer.multihead_attn.in_proj_weight[:512, :].T) # use same parameter values\n",
    "my_decoder_layer.mha.attn[0].W_K = nn.Parameter(pt_decoder_layer.multihead_attn.in_proj_weight[512:2*512, :].T)\n",
    "my_decoder_layer.mha.attn[0].W_V = nn.Parameter(pt_decoder_layer.multihead_attn.in_proj_weight[2*512:3*512, :].T)\n",
    "my_decoder_layer.mha.W_O = nn.Parameter(pt_decoder_layer.multihead_attn.out_proj.weight.T)\n",
    "\n",
    "# transfer linear layer parameters to my layer\n",
    "my_decoder_layer.linear1.weight = nn.Parameter(pt_decoder_layer.linear1.weight)\n",
    "my_decoder_layer.linear1.bias = nn.Parameter(pt_decoder_layer.linear1.bias)\n",
    "\n",
    "my_decoder_layer.linear2.weight = nn.Parameter(pt_decoder_layer.linear2.weight)\n",
    "my_decoder_layer.linear2.bias = nn.Parameter(pt_decoder_layer.linear2.bias)\n",
    "\n",
    "my_dec_out = my_decoder_layer(target, memory)\n",
    "\n",
    "assert(torch.allclose(my_dec_out, pt_dec_out, atol=1e-3, rtol=1e-3))\n",
    "\n",
    "# test 2 - transformer decoder with causal masking\n",
    "pt_dec_out = pt_decoder_layer(target, memory, tgt_mask = nn.Transformer.generate_square_subsequent_mask(8), tgt_is_causal=True)\n",
    "my_decoder_layer.mmha.attn[0].use_mask = True\n",
    "\n",
    "my_dec_out = my_decoder_layer(target, memory)\n",
    "\n",
    "assert(torch.allclose(my_dec_out, pt_dec_out, atol=1e-3, rtol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all the separate components together to make the transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_encoder = 6, n_decoder = 6, d_model = 16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(OrderedDict({f\"enc_layer_{i}\": EncoderLayer(d_model) for i in range(n_encoder)}))\n",
    "        self.decoder = nn.Sequential(OrderedDict({f\"dec_layer_{i}\": DecoderLayer(d_model) for i in range(n_decoder)}))\n",
    "        self.readout = nn.Linear(d_model, d_model)\n",
    "        self.pos_encoder = PositionalEncoder(seq_len, d_model)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x_input, x_output):\n",
    "        x_input = x_input + self.pos_encoder()\n",
    "        enc_embed = self.encoder(x_input)\n",
    "        _, dec_embed = self.decoder((enc_embed, x_output + self.pos_encoder()))\n",
    "\n",
    "        raise ValueError()\n",
    "        return self.readout(dec_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selective copying task to test the model\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_selective_copying_data(num_samples):\n",
    "    vocab_size = 16\n",
    "    seq_len = 64\n",
    "    num_data_tokens = 64 #16\n",
    "\n",
    "    data = np.random.randint(1, vocab_size, size=(num_samples, num_data_tokens))\n",
    "\n",
    "    inputs = np.zeros((num_samples, seq_len), dtype=int)\n",
    "    targets = np.zeros((num_samples, seq_len), dtype=int)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        indices = (np.random.permutation(seq_len)[:num_data_tokens])\n",
    "        indices = np.sort(indices)\n",
    "\n",
    "        inputs[i, indices] = data[i]\n",
    "        targets[i, :num_data_tokens] = data[i]\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "class SelectiveCopyingDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = F.one_hot(torch.tensor(inputs, dtype=torch.long))\n",
    "        self.targets = F.one_hot(torch.tensor(targets, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.permute(self.targets[idx], (0, 1)), torch.permute(self.targets[idx], (0, 1)) #  torch.permute(self.inputs[idx], (0, 1)),\n",
    "    \n",
    "def create_dataloader(inputs, targets, batch_size):\n",
    "    dataset = SelectiveCopyingDataset(inputs, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "inputs, targets = generate_selective_copying_data(1000)\n",
    "dataloader = create_dataloader(inputs, targets, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_encoder = 6\n",
    "n_decoder = 6\n",
    "d_model = 16\n",
    "seq_len = 64\n",
    "model = Transformer(n_encoder, n_decoder, d_model, seq_len)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = nn.MultiheadAttention(4, 4, dropout=0.0, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5956e-02, 6.6118e-01, 1.8398e-01,  ..., 4.5238e-05,\n",
      "          3.9190e-03, 1.3030e-02],\n",
      "         [8.2817e-03, 6.0283e-01, 3.2025e-01,  ..., 1.4117e-05,\n",
      "          7.5632e-04, 6.0450e-03],\n",
      "         [1.2014e-02, 6.1561e-01, 2.7789e-01,  ..., 2.9495e-05,\n",
      "          1.4862e-03, 8.5560e-03],\n",
      "         ...,\n",
      "         [1.5982e-02, 5.0261e-01, 2.6106e-01,  ..., 1.1692e-04,\n",
      "          6.9093e-03, 1.6994e-02],\n",
      "         [1.9128e-02, 4.8288e-01, 1.6775e-01,  ..., 1.6155e-04,\n",
      "          1.6756e-02, 2.1730e-02],\n",
      "         [1.5123e-02, 5.6646e-01, 2.1201e-01,  ..., 6.5839e-05,\n",
      "          6.7349e-03, 1.4115e-02]],\n",
      "\n",
      "        [[1.3821e-01, 6.1754e-01, 8.0191e-02,  ..., 6.2129e-05,\n",
      "          1.7153e-03, 2.4272e-02],\n",
      "         [1.3089e-01, 6.3429e-01, 1.0406e-01,  ..., 2.5829e-05,\n",
      "          7.3846e-04, 1.2821e-02],\n",
      "         [1.3619e-01, 6.1736e-01, 1.0914e-01,  ..., 3.4328e-05,\n",
      "          7.3727e-04, 1.2684e-02],\n",
      "         ...,\n",
      "         [1.3129e-01, 5.6432e-01, 9.0591e-02,  ..., 1.3236e-04,\n",
      "          5.0031e-03, 3.8740e-02],\n",
      "         [1.3128e-01, 5.1454e-01, 8.4174e-02,  ..., 2.2848e-04,\n",
      "          8.8855e-03, 5.4875e-02],\n",
      "         [1.2780e-01, 6.0766e-01, 9.2235e-02,  ..., 5.9346e-05,\n",
      "          2.7370e-03, 2.7681e-02]],\n",
      "\n",
      "        [[2.7848e-02, 5.9888e-01, 2.6367e-01,  ..., 1.9510e-05,\n",
      "          1.5846e-03, 6.3626e-03],\n",
      "         [2.5624e-02, 5.2860e-01, 3.5749e-01,  ..., 2.2460e-05,\n",
      "          1.0003e-03, 4.5090e-03],\n",
      "         [4.0596e-02, 5.7911e-01, 2.7352e-01,  ..., 3.4878e-05,\n",
      "          1.5161e-03, 7.0097e-03],\n",
      "         ...,\n",
      "         [8.0430e-02, 5.8918e-01, 1.2930e-01,  ..., 7.8954e-05,\n",
      "          4.0869e-03, 1.8845e-02],\n",
      "         [8.1781e-02, 6.0111e-01, 1.5673e-01,  ..., 5.8503e-05,\n",
      "          2.4385e-03, 1.5267e-02],\n",
      "         [4.5208e-02, 5.8304e-01, 2.3674e-01,  ..., 3.6088e-05,\n",
      "          1.8393e-03, 1.0519e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.2253e-02, 9.2794e-01, 6.4907e-03,  ..., 3.7954e-05,\n",
      "          1.5888e-03, 6.1847e-03],\n",
      "         [2.0264e-02, 9.3857e-01, 6.9754e-03,  ..., 2.5812e-05,\n",
      "          1.2278e-03, 5.1800e-03],\n",
      "         [2.8775e-02, 9.0331e-01, 1.1465e-02,  ..., 7.2112e-05,\n",
      "          2.2630e-03, 8.2622e-03],\n",
      "         ...,\n",
      "         [5.0390e-02, 8.0409e-01, 9.8972e-03,  ..., 3.2232e-04,\n",
      "          6.2672e-03, 1.4479e-02],\n",
      "         [4.3051e-02, 8.3287e-01, 6.4085e-03,  ..., 2.0272e-04,\n",
      "          5.0011e-03, 1.1952e-02],\n",
      "         [4.1163e-02, 8.6064e-01, 5.7704e-03,  ..., 1.3498e-04,\n",
      "          4.2907e-03, 1.0720e-02]],\n",
      "\n",
      "        [[1.1088e-01, 6.7409e-01, 1.2490e-01,  ..., 1.3195e-04,\n",
      "          6.5436e-04, 7.4122e-03],\n",
      "         [3.2499e-02, 7.5298e-01, 1.8039e-01,  ..., 3.9418e-05,\n",
      "          9.7034e-05, 1.8605e-03],\n",
      "         [5.1888e-02, 7.2385e-01, 1.6989e-01,  ..., 8.5149e-05,\n",
      "          2.6216e-04, 3.3962e-03],\n",
      "         ...,\n",
      "         [2.1622e-01, 5.5742e-01, 8.1668e-02,  ..., 3.0984e-04,\n",
      "          2.2375e-03, 1.0648e-02],\n",
      "         [1.9324e-01, 5.5551e-01, 9.2117e-02,  ..., 3.9012e-04,\n",
      "          3.1067e-03, 1.1537e-02],\n",
      "         [1.1827e-01, 6.9167e-01, 1.0507e-01,  ..., 1.2437e-04,\n",
      "          8.8871e-04, 4.6635e-03]],\n",
      "\n",
      "        [[3.6052e-01, 2.8273e-01, 7.1859e-02,  ..., 3.8890e-04,\n",
      "          2.0160e-03, 2.7257e-02],\n",
      "         [2.7455e-01, 3.7069e-01, 1.1327e-01,  ..., 1.8494e-04,\n",
      "          9.1214e-04, 1.8199e-02],\n",
      "         [2.1623e-01, 3.9163e-01, 1.2060e-01,  ..., 2.1962e-04,\n",
      "          1.1055e-03, 2.3344e-02],\n",
      "         ...,\n",
      "         [4.0810e-01, 2.4774e-01, 3.7335e-02,  ..., 5.0031e-04,\n",
      "          5.8053e-03, 3.0876e-02],\n",
      "         [3.8256e-01, 2.9850e-01, 4.5070e-02,  ..., 2.5847e-04,\n",
      "          3.1739e-03, 2.5002e-02],\n",
      "         [4.4672e-01, 2.6533e-01, 4.0465e-02,  ..., 2.4415e-04,\n",
      "          2.8021e-03, 2.0974e-02]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[7.5370e-04, 9.4803e-01, 1.4316e-02,  ..., 2.2609e-07,\n",
      "          4.0883e-06, 1.6023e-04],\n",
      "         [6.8958e-04, 9.4234e-01, 1.1421e-02,  ..., 3.8207e-07,\n",
      "          9.4839e-06, 5.7500e-04],\n",
      "         [7.8071e-04, 9.3727e-01, 1.5645e-02,  ..., 3.9667e-07,\n",
      "          6.5574e-06, 3.0738e-04],\n",
      "         ...,\n",
      "         [1.0871e-03, 9.3236e-01, 1.6365e-02,  ..., 5.8836e-07,\n",
      "          9.9800e-06, 3.5911e-04],\n",
      "         [7.8211e-04, 9.4012e-01, 1.3261e-02,  ..., 2.6532e-07,\n",
      "          4.5421e-06, 2.0581e-04],\n",
      "         [6.6551e-04, 9.5530e-01, 1.0012e-02,  ..., 2.1188e-07,\n",
      "          6.3415e-06, 3.1561e-04]],\n",
      "\n",
      "        [[2.2040e-03, 7.7175e-01, 1.7789e-01,  ..., 2.6971e-09,\n",
      "          1.2282e-04, 1.7028e-03],\n",
      "         [2.7495e-03, 7.7112e-01, 1.6558e-01,  ..., 4.2838e-09,\n",
      "          1.7010e-04, 2.6730e-03],\n",
      "         [3.6068e-03, 7.3419e-01, 1.6952e-01,  ..., 1.6057e-08,\n",
      "          2.9806e-04, 4.5703e-03],\n",
      "         ...,\n",
      "         [5.3590e-03, 7.3242e-01, 1.8988e-01,  ..., 3.1779e-08,\n",
      "          4.1307e-04, 4.2546e-03],\n",
      "         [2.7435e-03, 7.3087e-01, 1.8140e-01,  ..., 1.1414e-08,\n",
      "          2.1989e-04, 3.1473e-03],\n",
      "         [2.8678e-03, 7.7358e-01, 1.5876e-01,  ..., 4.2962e-09,\n",
      "          1.7799e-04, 2.9287e-03]],\n",
      "\n",
      "        [[1.0420e-02, 7.8681e-01, 4.3043e-02,  ..., 1.4875e-07,\n",
      "          1.2511e-04, 4.1828e-03],\n",
      "         [1.2724e-02, 8.2849e-01, 3.6677e-02,  ..., 1.0583e-07,\n",
      "          1.2118e-04, 3.7966e-03],\n",
      "         [1.1998e-02, 7.8562e-01, 4.5396e-02,  ..., 2.1446e-07,\n",
      "          1.5774e-04, 4.7869e-03],\n",
      "         ...,\n",
      "         [1.7468e-02, 7.8302e-01, 4.7156e-02,  ..., 3.0670e-07,\n",
      "          2.1461e-04, 5.1914e-03],\n",
      "         [9.1226e-03, 8.0224e-01, 3.3286e-02,  ..., 2.3767e-07,\n",
      "          1.6344e-04, 5.1202e-03],\n",
      "         [9.6347e-03, 8.3033e-01, 3.0774e-02,  ..., 1.5452e-07,\n",
      "          1.3915e-04, 4.5051e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.1633e-04, 8.7470e-01, 3.7999e-02,  ..., 3.8656e-07,\n",
      "          6.5007e-06, 5.0159e-04],\n",
      "         [1.4229e-04, 8.5036e-01, 3.7420e-02,  ..., 3.8109e-07,\n",
      "          4.2378e-06, 7.4352e-04],\n",
      "         [2.4767e-04, 8.3614e-01, 4.3629e-02,  ..., 9.8199e-07,\n",
      "          1.0273e-05, 1.0912e-03],\n",
      "         ...,\n",
      "         [2.2311e-04, 8.3528e-01, 4.8384e-02,  ..., 4.6420e-07,\n",
      "          7.0966e-06, 5.0860e-04],\n",
      "         [1.7541e-04, 7.8996e-01, 5.5069e-02,  ..., 3.3065e-07,\n",
      "          4.6634e-06, 4.1751e-04],\n",
      "         [1.3683e-04, 8.7486e-01, 3.2571e-02,  ..., 3.0868e-07,\n",
      "          3.8292e-06, 6.6862e-04]],\n",
      "\n",
      "        [[1.2000e-03, 8.5539e-01, 1.2446e-01,  ..., 1.1611e-07,\n",
      "          4.6104e-05, 5.8102e-06],\n",
      "         [1.8937e-03, 8.3823e-01, 1.3698e-01,  ..., 2.1653e-07,\n",
      "          7.4391e-05, 1.0430e-05],\n",
      "         [2.3645e-03, 8.0139e-01, 1.6293e-01,  ..., 4.4964e-07,\n",
      "          1.4289e-04, 2.6157e-05],\n",
      "         ...,\n",
      "         [1.9168e-03, 8.1594e-01, 1.5078e-01,  ..., 3.9062e-07,\n",
      "          1.1568e-04, 1.9749e-05],\n",
      "         [1.6451e-03, 8.1380e-01, 1.5252e-01,  ..., 3.6774e-07,\n",
      "          1.1188e-04, 1.9515e-05],\n",
      "         [3.8153e-03, 7.9974e-01, 1.6564e-01,  ..., 5.4095e-07,\n",
      "          1.5389e-04, 2.5106e-05]],\n",
      "\n",
      "        [[1.2703e-02, 4.8795e-01, 1.3951e-02,  ..., 3.6472e-06,\n",
      "          1.0422e-04, 9.9402e-04],\n",
      "         [9.5973e-03, 5.0920e-01, 1.1448e-02,  ..., 4.1711e-06,\n",
      "          1.1618e-04, 1.6789e-03],\n",
      "         [3.3955e-02, 5.6240e-01, 1.4070e-02,  ..., 2.0450e-05,\n",
      "          3.8954e-04, 5.0084e-03],\n",
      "         ...,\n",
      "         [2.3697e-02, 4.9124e-01, 1.7605e-02,  ..., 1.4735e-05,\n",
      "          2.8861e-04, 2.5574e-03],\n",
      "         [1.3340e-02, 5.1823e-01, 1.2506e-02,  ..., 4.7129e-06,\n",
      "          1.2848e-04, 1.5580e-03],\n",
      "         [1.6485e-02, 5.5959e-01, 1.0973e-02,  ..., 5.2542e-06,\n",
      "          1.4430e-04, 2.3211e-03]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[2.3090e-03, 4.5413e-03, 1.7686e-01,  ..., 1.4053e-02,\n",
      "          5.2276e-03, 1.5950e-01],\n",
      "         [2.0141e-03, 5.3062e-03, 1.8830e-01,  ..., 1.3114e-02,\n",
      "          4.0482e-03, 1.4373e-01],\n",
      "         [1.7098e-03, 7.3193e-03, 2.2162e-01,  ..., 1.0004e-02,\n",
      "          2.4088e-03, 1.0436e-01],\n",
      "         ...,\n",
      "         [3.1432e-03, 8.1456e-03, 1.8630e-01,  ..., 1.0487e-02,\n",
      "          3.0191e-03, 1.1944e-01],\n",
      "         [4.1227e-03, 5.9272e-03, 1.7324e-01,  ..., 1.2085e-02,\n",
      "          5.1463e-03, 1.5036e-01],\n",
      "         [2.5699e-03, 7.5375e-03, 2.0538e-01,  ..., 9.6821e-03,\n",
      "          2.6715e-03, 1.1601e-01]],\n",
      "\n",
      "        [[3.0691e-04, 5.6947e-02, 2.4888e-03,  ..., 9.0331e-04,\n",
      "          7.7224e-03, 4.3012e-01],\n",
      "         [1.4774e-04, 6.8601e-02, 2.9108e-03,  ..., 2.1282e-04,\n",
      "          4.8397e-03, 3.7887e-01],\n",
      "         [1.6447e-04, 6.0945e-02, 3.3231e-03,  ..., 3.7708e-04,\n",
      "          7.6633e-03, 4.2258e-01],\n",
      "         ...,\n",
      "         [3.5856e-04, 6.8283e-02, 2.4249e-03,  ..., 6.1700e-04,\n",
      "          5.2869e-03, 3.7705e-01],\n",
      "         [2.0355e-04, 6.7224e-02, 2.0279e-03,  ..., 3.2807e-04,\n",
      "          4.3445e-03, 3.9829e-01],\n",
      "         [1.4193e-04, 7.7547e-02, 2.2679e-03,  ..., 1.2741e-04,\n",
      "          2.9148e-03, 3.4212e-01]],\n",
      "\n",
      "        [[1.4583e-04, 4.1016e-02, 4.2891e-02,  ..., 2.2887e-04,\n",
      "          9.9533e-04, 6.1861e-01],\n",
      "         [1.1011e-04, 5.1212e-02, 5.4892e-02,  ..., 1.1375e-04,\n",
      "          6.7268e-04, 5.8704e-01],\n",
      "         [1.2125e-04, 5.1970e-02, 5.8465e-02,  ..., 1.2428e-04,\n",
      "          7.5765e-04, 5.8468e-01],\n",
      "         ...,\n",
      "         [2.2899e-04, 5.5545e-02, 4.6635e-02,  ..., 2.8521e-04,\n",
      "          1.0072e-03, 5.2924e-01],\n",
      "         [1.0076e-04, 4.1509e-02, 3.6052e-02,  ..., 1.4849e-04,\n",
      "          5.9969e-04, 6.0886e-01],\n",
      "         [9.3610e-05, 6.1585e-02, 5.2594e-02,  ..., 7.2125e-05,\n",
      "          4.1246e-04, 5.3102e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.7353e-03, 9.9895e-03, 1.9679e-02,  ..., 2.8351e-03,\n",
      "          7.5189e-03, 4.6381e-02],\n",
      "         [9.6936e-04, 9.4737e-03, 2.4783e-02,  ..., 2.8563e-03,\n",
      "          9.7815e-03, 4.5513e-02],\n",
      "         [6.7762e-04, 8.3729e-03, 1.9089e-02,  ..., 2.1156e-03,\n",
      "          8.3726e-03, 3.7858e-02],\n",
      "         ...,\n",
      "         [1.5753e-03, 1.1145e-02, 1.9085e-02,  ..., 1.6998e-03,\n",
      "          4.9043e-03, 3.4779e-02],\n",
      "         [1.4836e-03, 8.0647e-03, 1.3061e-02,  ..., 1.5935e-03,\n",
      "          4.1680e-03, 3.6031e-02],\n",
      "         [1.0205e-03, 1.0563e-02, 1.8285e-02,  ..., 1.3933e-03,\n",
      "          4.9131e-03, 3.0292e-02]],\n",
      "\n",
      "        [[4.7707e-04, 8.0285e-03, 2.3696e-02,  ..., 1.1600e-03,\n",
      "          3.2654e-02, 5.8466e-01],\n",
      "         [2.7250e-04, 8.1758e-03, 3.7740e-02,  ..., 1.0543e-03,\n",
      "          4.0004e-02, 4.5981e-01],\n",
      "         [3.0716e-04, 1.1217e-02, 5.1710e-02,  ..., 1.1796e-03,\n",
      "          4.8109e-02, 3.6668e-01],\n",
      "         ...,\n",
      "         [5.4345e-04, 1.1484e-02, 3.3677e-02,  ..., 1.1744e-03,\n",
      "          3.0691e-02, 5.1385e-01],\n",
      "         [4.5315e-04, 1.5891e-02, 4.8510e-02,  ..., 8.4072e-04,\n",
      "          2.3548e-02, 4.4378e-01],\n",
      "         [3.6938e-04, 1.6931e-02, 6.3781e-02,  ..., 8.5656e-04,\n",
      "          2.6582e-02, 3.6008e-01]],\n",
      "\n",
      "        [[3.3165e-04, 2.2287e-02, 5.4575e-02,  ..., 1.2074e-02,\n",
      "          9.0868e-02, 7.5608e-02],\n",
      "         [7.9872e-05, 1.8554e-02, 3.4923e-02,  ..., 5.5264e-03,\n",
      "          5.8590e-02, 4.6758e-02],\n",
      "         [1.4419e-04, 2.4659e-02, 3.6840e-02,  ..., 5.9816e-03,\n",
      "          5.3103e-02, 4.3484e-02],\n",
      "         ...,\n",
      "         [2.8272e-04, 1.9358e-02, 4.7732e-02,  ..., 7.8676e-03,\n",
      "          6.5930e-02, 5.6171e-02],\n",
      "         [1.4147e-04, 1.6633e-02, 3.4714e-02,  ..., 4.0618e-03,\n",
      "          4.1610e-02, 3.5435e-02],\n",
      "         [1.5650e-04, 2.0653e-02, 3.7788e-02,  ..., 5.0999e-03,\n",
      "          4.9122e-02, 4.1164e-02]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.6670e-01, 1.7755e-03, 5.7122e-04,  ..., 5.7988e-04,\n",
      "          1.5069e-01, 1.8247e-03],\n",
      "         [1.4341e-01, 5.5959e-04, 1.5676e-04,  ..., 3.8219e-04,\n",
      "          1.8911e-01, 9.5822e-04],\n",
      "         [1.0128e-01, 5.6107e-04, 1.6087e-04,  ..., 2.8689e-04,\n",
      "          1.3421e-01, 6.7422e-04],\n",
      "         ...,\n",
      "         [9.1684e-02, 1.1741e-03, 4.6498e-04,  ..., 1.1939e-03,\n",
      "          1.8604e-01, 2.9959e-03],\n",
      "         [9.0187e-02, 1.3049e-03, 5.7174e-04,  ..., 2.0301e-03,\n",
      "          2.2397e-01, 5.0502e-03],\n",
      "         [5.4818e-02, 7.3273e-04, 3.1150e-04,  ..., 1.0021e-03,\n",
      "          1.7449e-01, 2.8245e-03]],\n",
      "\n",
      "        [[3.3966e-02, 4.5388e-02, 1.7113e-03,  ..., 1.0468e-02,\n",
      "          2.5207e-02, 5.7605e-02],\n",
      "         [2.9152e-02, 5.2197e-02, 1.1352e-03,  ..., 6.4724e-03,\n",
      "          1.5938e-02, 4.9316e-02],\n",
      "         [3.1668e-02, 5.8613e-02, 1.4171e-03,  ..., 7.9211e-03,\n",
      "          1.7877e-02, 5.3526e-02],\n",
      "         ...,\n",
      "         [3.2965e-02, 3.0243e-02, 1.2854e-03,  ..., 1.3881e-02,\n",
      "          3.0420e-02, 6.2064e-02],\n",
      "         [2.7589e-02, 1.8848e-02, 1.1608e-03,  ..., 1.4890e-02,\n",
      "          4.1950e-02, 5.5344e-02],\n",
      "         [2.9986e-02, 3.0627e-02, 8.8992e-04,  ..., 8.9577e-03,\n",
      "          2.2133e-02, 5.5418e-02]],\n",
      "\n",
      "        [[1.1501e-01, 1.5344e-02, 3.4314e-04,  ..., 6.4974e-05,\n",
      "          1.4755e-03, 1.7258e-02],\n",
      "         [1.0291e-01, 2.1599e-02, 3.8315e-04,  ..., 4.5537e-05,\n",
      "          1.2299e-03, 1.7365e-02],\n",
      "         [7.1896e-02, 1.1190e-02, 2.2594e-04,  ..., 2.9191e-05,\n",
      "          8.8287e-04, 1.3071e-02],\n",
      "         ...,\n",
      "         [4.6568e-02, 1.8093e-02, 9.4307e-04,  ..., 6.6185e-04,\n",
      "          6.1579e-03, 2.8027e-02],\n",
      "         [5.8118e-02, 1.1596e-02, 4.5998e-04,  ..., 4.0606e-04,\n",
      "          4.4283e-03, 2.2221e-02],\n",
      "         [4.7246e-02, 1.3975e-02, 3.4657e-04,  ..., 1.4583e-04,\n",
      "          2.4933e-03, 1.9058e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[4.2429e-01, 1.8369e-03, 1.0020e-03,  ..., 7.8419e-04,\n",
      "          1.1615e-02, 2.1225e-03],\n",
      "         [4.7103e-01, 2.3469e-03, 1.0092e-03,  ..., 6.4129e-04,\n",
      "          9.7363e-03, 2.0948e-03],\n",
      "         [5.6021e-01, 3.5408e-03, 1.2471e-03,  ..., 4.4551e-04,\n",
      "          5.7538e-03, 1.6179e-03],\n",
      "         ...,\n",
      "         [3.0452e-01, 5.0117e-03, 2.9432e-03,  ..., 3.4955e-03,\n",
      "          2.7929e-02, 7.6801e-03],\n",
      "         [2.7599e-01, 2.4291e-03, 1.8913e-03,  ..., 2.8234e-03,\n",
      "          2.8204e-02, 5.3273e-03],\n",
      "         [1.9165e-01, 1.8488e-03, 1.0201e-03,  ..., 3.0152e-03,\n",
      "          2.4219e-02, 5.7757e-03]],\n",
      "\n",
      "        [[6.8584e-01, 1.6056e-02, 1.0727e-04,  ..., 1.4301e-04,\n",
      "          9.6988e-03, 7.2120e-02],\n",
      "         [7.8542e-01, 9.3286e-03, 2.4729e-05,  ..., 2.9824e-05,\n",
      "          3.6456e-03, 4.6782e-02],\n",
      "         [7.3100e-01, 1.2215e-02, 5.1195e-05,  ..., 5.2682e-05,\n",
      "          3.9109e-03, 5.3485e-02],\n",
      "         ...,\n",
      "         [4.8722e-01, 1.6076e-02, 2.4354e-04,  ..., 5.0361e-04,\n",
      "          1.4188e-02, 7.4769e-02],\n",
      "         [4.5334e-01, 9.5100e-03, 1.1166e-04,  ..., 5.2756e-04,\n",
      "          2.2072e-02, 7.1782e-02],\n",
      "         [5.4364e-01, 7.4126e-03, 4.4983e-05,  ..., 1.4834e-04,\n",
      "          7.8149e-03, 5.1080e-02]],\n",
      "\n",
      "        [[7.1010e-01, 2.7689e-03, 2.8803e-03,  ..., 6.8069e-05,\n",
      "          2.3050e-02, 2.5292e-03],\n",
      "         [7.2856e-01, 1.4702e-03, 1.5572e-03,  ..., 2.0971e-05,\n",
      "          1.6188e-02, 1.3291e-03],\n",
      "         [7.3055e-01, 9.5440e-04, 9.5249e-04,  ..., 1.3643e-05,\n",
      "          1.4723e-02, 9.0784e-04],\n",
      "         ...,\n",
      "         [5.0437e-01, 4.3200e-03, 3.3662e-03,  ..., 1.7650e-04,\n",
      "          3.2515e-02, 4.8575e-03],\n",
      "         [5.5358e-01, 2.7390e-03, 1.6421e-03,  ..., 1.0288e-04,\n",
      "          3.0403e-02, 3.6303e-03],\n",
      "         [4.5127e-01, 2.6857e-03, 1.2643e-03,  ..., 1.1306e-04,\n",
      "          3.1813e-02, 4.1866e-03]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0125, 0.0134, 0.0126,  ..., 0.0131, 0.0174, 0.0169],\n",
      "         [0.0127, 0.0136, 0.0127,  ..., 0.0132, 0.0173, 0.0168],\n",
      "         [0.0127, 0.0136, 0.0126,  ..., 0.0131, 0.0175, 0.0169],\n",
      "         ...,\n",
      "         [0.0122, 0.0133, 0.0125,  ..., 0.0128, 0.0173, 0.0170],\n",
      "         [0.0121, 0.0132, 0.0125,  ..., 0.0129, 0.0172, 0.0170],\n",
      "         [0.0122, 0.0133, 0.0125,  ..., 0.0130, 0.0172, 0.0169]],\n",
      "\n",
      "        [[0.0119, 0.0116, 0.0116,  ..., 0.0151, 0.0169, 0.0151],\n",
      "         [0.0121, 0.0118, 0.0117,  ..., 0.0151, 0.0168, 0.0152],\n",
      "         [0.0122, 0.0119, 0.0118,  ..., 0.0150, 0.0169, 0.0152],\n",
      "         ...,\n",
      "         [0.0119, 0.0116, 0.0115,  ..., 0.0151, 0.0168, 0.0151],\n",
      "         [0.0118, 0.0116, 0.0116,  ..., 0.0151, 0.0169, 0.0152],\n",
      "         [0.0120, 0.0117, 0.0116,  ..., 0.0152, 0.0167, 0.0151]],\n",
      "\n",
      "        [[0.0109, 0.0134, 0.0120,  ..., 0.0137, 0.0160, 0.0150],\n",
      "         [0.0112, 0.0135, 0.0121,  ..., 0.0139, 0.0160, 0.0150],\n",
      "         [0.0110, 0.0134, 0.0120,  ..., 0.0138, 0.0161, 0.0150],\n",
      "         ...,\n",
      "         [0.0106, 0.0134, 0.0118,  ..., 0.0137, 0.0159, 0.0150],\n",
      "         [0.0109, 0.0136, 0.0120,  ..., 0.0138, 0.0159, 0.0150],\n",
      "         [0.0108, 0.0135, 0.0119,  ..., 0.0138, 0.0160, 0.0150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0130, 0.0113, 0.0123,  ..., 0.0156, 0.0142, 0.0171],\n",
      "         [0.0131, 0.0115, 0.0125,  ..., 0.0155, 0.0143, 0.0170],\n",
      "         [0.0132, 0.0115, 0.0125,  ..., 0.0155, 0.0143, 0.0170],\n",
      "         ...,\n",
      "         [0.0131, 0.0114, 0.0124,  ..., 0.0156, 0.0143, 0.0171],\n",
      "         [0.0129, 0.0112, 0.0121,  ..., 0.0158, 0.0143, 0.0172],\n",
      "         [0.0131, 0.0113, 0.0121,  ..., 0.0158, 0.0142, 0.0172]],\n",
      "\n",
      "        [[0.0130, 0.0111, 0.0111,  ..., 0.0128, 0.0171, 0.0179],\n",
      "         [0.0132, 0.0112, 0.0112,  ..., 0.0129, 0.0172, 0.0177],\n",
      "         [0.0133, 0.0113, 0.0112,  ..., 0.0129, 0.0172, 0.0177],\n",
      "         ...,\n",
      "         [0.0130, 0.0110, 0.0110,  ..., 0.0127, 0.0172, 0.0179],\n",
      "         [0.0128, 0.0109, 0.0110,  ..., 0.0127, 0.0172, 0.0179],\n",
      "         [0.0130, 0.0110, 0.0110,  ..., 0.0128, 0.0170, 0.0180]],\n",
      "\n",
      "        [[0.0130, 0.0126, 0.0130,  ..., 0.0150, 0.0138, 0.0167],\n",
      "         [0.0133, 0.0127, 0.0131,  ..., 0.0151, 0.0139, 0.0166],\n",
      "         [0.0132, 0.0126, 0.0130,  ..., 0.0151, 0.0139, 0.0166],\n",
      "         ...,\n",
      "         [0.0129, 0.0125, 0.0130,  ..., 0.0149, 0.0137, 0.0167],\n",
      "         [0.0128, 0.0125, 0.0129,  ..., 0.0149, 0.0137, 0.0167],\n",
      "         [0.0129, 0.0125, 0.0130,  ..., 0.0150, 0.0138, 0.0167]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0151, 0.0162, 0.0149,  ..., 0.0147, 0.0149, 0.0148],\n",
      "         [0.0152, 0.0163, 0.0150,  ..., 0.0147, 0.0150, 0.0148],\n",
      "         [0.0152, 0.0164, 0.0150,  ..., 0.0148, 0.0150, 0.0149],\n",
      "         ...,\n",
      "         [0.0152, 0.0162, 0.0151,  ..., 0.0147, 0.0147, 0.0147],\n",
      "         [0.0152, 0.0161, 0.0150,  ..., 0.0147, 0.0149, 0.0148],\n",
      "         [0.0153, 0.0163, 0.0151,  ..., 0.0149, 0.0150, 0.0148]],\n",
      "\n",
      "        [[0.0158, 0.0164, 0.0147,  ..., 0.0134, 0.0141, 0.0154],\n",
      "         [0.0158, 0.0166, 0.0148,  ..., 0.0132, 0.0140, 0.0153],\n",
      "         [0.0159, 0.0167, 0.0147,  ..., 0.0131, 0.0139, 0.0154],\n",
      "         ...,\n",
      "         [0.0157, 0.0165, 0.0149,  ..., 0.0135, 0.0141, 0.0154],\n",
      "         [0.0157, 0.0165, 0.0149,  ..., 0.0136, 0.0141, 0.0154],\n",
      "         [0.0157, 0.0165, 0.0150,  ..., 0.0135, 0.0141, 0.0153]],\n",
      "\n",
      "        [[0.0150, 0.0168, 0.0151,  ..., 0.0141, 0.0144, 0.0157],\n",
      "         [0.0150, 0.0167, 0.0150,  ..., 0.0141, 0.0146, 0.0158],\n",
      "         [0.0151, 0.0167, 0.0151,  ..., 0.0141, 0.0147, 0.0158],\n",
      "         ...,\n",
      "         [0.0151, 0.0165, 0.0151,  ..., 0.0143, 0.0147, 0.0157],\n",
      "         [0.0151, 0.0165, 0.0150,  ..., 0.0142, 0.0146, 0.0157],\n",
      "         [0.0152, 0.0165, 0.0152,  ..., 0.0142, 0.0146, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0152, 0.0162, 0.0149,  ..., 0.0143, 0.0139, 0.0148],\n",
      "         [0.0152, 0.0164, 0.0149,  ..., 0.0141, 0.0137, 0.0147],\n",
      "         [0.0152, 0.0165, 0.0149,  ..., 0.0140, 0.0137, 0.0145],\n",
      "         ...,\n",
      "         [0.0152, 0.0163, 0.0148,  ..., 0.0143, 0.0140, 0.0147],\n",
      "         [0.0153, 0.0163, 0.0149,  ..., 0.0142, 0.0141, 0.0147],\n",
      "         [0.0153, 0.0164, 0.0149,  ..., 0.0143, 0.0141, 0.0148]],\n",
      "\n",
      "        [[0.0164, 0.0170, 0.0161,  ..., 0.0145, 0.0143, 0.0143],\n",
      "         [0.0167, 0.0173, 0.0163,  ..., 0.0145, 0.0143, 0.0143],\n",
      "         [0.0168, 0.0175, 0.0164,  ..., 0.0145, 0.0143, 0.0142],\n",
      "         ...,\n",
      "         [0.0165, 0.0171, 0.0162,  ..., 0.0145, 0.0142, 0.0143],\n",
      "         [0.0165, 0.0170, 0.0162,  ..., 0.0146, 0.0144, 0.0144],\n",
      "         [0.0166, 0.0171, 0.0162,  ..., 0.0147, 0.0144, 0.0144]],\n",
      "\n",
      "        [[0.0160, 0.0161, 0.0151,  ..., 0.0136, 0.0151, 0.0150],\n",
      "         [0.0163, 0.0162, 0.0151,  ..., 0.0134, 0.0151, 0.0148],\n",
      "         [0.0164, 0.0162, 0.0151,  ..., 0.0136, 0.0152, 0.0149],\n",
      "         ...,\n",
      "         [0.0163, 0.0161, 0.0151,  ..., 0.0136, 0.0151, 0.0148],\n",
      "         [0.0160, 0.0160, 0.0152,  ..., 0.0139, 0.0151, 0.0149],\n",
      "         [0.0164, 0.0162, 0.0151,  ..., 0.0139, 0.0153, 0.0149]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0143, 0.0159, 0.0172,  ..., 0.0169, 0.0146, 0.0152],\n",
      "         [0.0143, 0.0158, 0.0170,  ..., 0.0169, 0.0146, 0.0151],\n",
      "         [0.0144, 0.0158, 0.0168,  ..., 0.0169, 0.0148, 0.0150],\n",
      "         ...,\n",
      "         [0.0144, 0.0158, 0.0169,  ..., 0.0170, 0.0148, 0.0150],\n",
      "         [0.0144, 0.0160, 0.0171,  ..., 0.0170, 0.0148, 0.0151],\n",
      "         [0.0145, 0.0159, 0.0169,  ..., 0.0171, 0.0150, 0.0149]],\n",
      "\n",
      "        [[0.0153, 0.0173, 0.0179,  ..., 0.0155, 0.0175, 0.0160],\n",
      "         [0.0152, 0.0172, 0.0178,  ..., 0.0156, 0.0173, 0.0160],\n",
      "         [0.0152, 0.0171, 0.0178,  ..., 0.0156, 0.0173, 0.0160],\n",
      "         ...,\n",
      "         [0.0155, 0.0174, 0.0176,  ..., 0.0155, 0.0176, 0.0161],\n",
      "         [0.0155, 0.0174, 0.0177,  ..., 0.0154, 0.0175, 0.0161],\n",
      "         [0.0154, 0.0172, 0.0176,  ..., 0.0155, 0.0175, 0.0161]],\n",
      "\n",
      "        [[0.0168, 0.0184, 0.0170,  ..., 0.0153, 0.0146, 0.0163],\n",
      "         [0.0168, 0.0184, 0.0170,  ..., 0.0154, 0.0148, 0.0163],\n",
      "         [0.0167, 0.0182, 0.0169,  ..., 0.0154, 0.0148, 0.0163],\n",
      "         ...,\n",
      "         [0.0168, 0.0183, 0.0170,  ..., 0.0154, 0.0150, 0.0163],\n",
      "         [0.0168, 0.0184, 0.0171,  ..., 0.0154, 0.0148, 0.0164],\n",
      "         [0.0168, 0.0183, 0.0169,  ..., 0.0154, 0.0150, 0.0163]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0159, 0.0170, 0.0155,  ..., 0.0174, 0.0151, 0.0150],\n",
      "         [0.0159, 0.0169, 0.0156,  ..., 0.0173, 0.0151, 0.0150],\n",
      "         [0.0159, 0.0170, 0.0156,  ..., 0.0173, 0.0151, 0.0151],\n",
      "         ...,\n",
      "         [0.0160, 0.0169, 0.0152,  ..., 0.0177, 0.0153, 0.0149],\n",
      "         [0.0159, 0.0170, 0.0154,  ..., 0.0175, 0.0151, 0.0150],\n",
      "         [0.0159, 0.0169, 0.0152,  ..., 0.0176, 0.0154, 0.0148]],\n",
      "\n",
      "        [[0.0151, 0.0179, 0.0177,  ..., 0.0167, 0.0181, 0.0154],\n",
      "         [0.0151, 0.0177, 0.0176,  ..., 0.0166, 0.0179, 0.0154],\n",
      "         [0.0151, 0.0176, 0.0175,  ..., 0.0166, 0.0177, 0.0154],\n",
      "         ...,\n",
      "         [0.0154, 0.0178, 0.0176,  ..., 0.0167, 0.0179, 0.0153],\n",
      "         [0.0154, 0.0178, 0.0177,  ..., 0.0167, 0.0180, 0.0153],\n",
      "         [0.0156, 0.0177, 0.0176,  ..., 0.0169, 0.0179, 0.0152]],\n",
      "\n",
      "        [[0.0149, 0.0173, 0.0172,  ..., 0.0149, 0.0170, 0.0156],\n",
      "         [0.0150, 0.0172, 0.0170,  ..., 0.0151, 0.0171, 0.0154],\n",
      "         [0.0151, 0.0171, 0.0169,  ..., 0.0151, 0.0170, 0.0154],\n",
      "         ...,\n",
      "         [0.0150, 0.0172, 0.0170,  ..., 0.0150, 0.0171, 0.0155],\n",
      "         [0.0151, 0.0171, 0.0169,  ..., 0.0151, 0.0172, 0.0154],\n",
      "         [0.0154, 0.0169, 0.0167,  ..., 0.0151, 0.0172, 0.0153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0149, 0.0153,  ..., 0.0149, 0.0157, 0.0157],\n",
      "         [0.0152, 0.0148, 0.0152,  ..., 0.0149, 0.0156, 0.0155],\n",
      "         [0.0153, 0.0148, 0.0153,  ..., 0.0150, 0.0154, 0.0153],\n",
      "         ...,\n",
      "         [0.0152, 0.0149, 0.0155,  ..., 0.0145, 0.0153, 0.0157],\n",
      "         [0.0153, 0.0152, 0.0156,  ..., 0.0145, 0.0153, 0.0158],\n",
      "         [0.0153, 0.0150, 0.0155,  ..., 0.0145, 0.0151, 0.0154]],\n",
      "\n",
      "        [[0.0150, 0.0148, 0.0149,  ..., 0.0158, 0.0157, 0.0152],\n",
      "         [0.0150, 0.0147, 0.0149,  ..., 0.0157, 0.0157, 0.0151],\n",
      "         [0.0151, 0.0149, 0.0150,  ..., 0.0157, 0.0156, 0.0152],\n",
      "         ...,\n",
      "         [0.0151, 0.0149, 0.0152,  ..., 0.0154, 0.0156, 0.0151],\n",
      "         [0.0152, 0.0151, 0.0154,  ..., 0.0152, 0.0155, 0.0150],\n",
      "         [0.0150, 0.0148, 0.0152,  ..., 0.0152, 0.0155, 0.0149]],\n",
      "\n",
      "        [[0.0148, 0.0151, 0.0152,  ..., 0.0156, 0.0155, 0.0153],\n",
      "         [0.0149, 0.0150, 0.0152,  ..., 0.0155, 0.0154, 0.0153],\n",
      "         [0.0150, 0.0151, 0.0153,  ..., 0.0154, 0.0154, 0.0152],\n",
      "         ...,\n",
      "         [0.0149, 0.0151, 0.0154,  ..., 0.0153, 0.0152, 0.0151],\n",
      "         [0.0148, 0.0151, 0.0154,  ..., 0.0154, 0.0152, 0.0152],\n",
      "         [0.0148, 0.0151, 0.0154,  ..., 0.0152, 0.0150, 0.0150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0153, 0.0149, 0.0152,  ..., 0.0155, 0.0160, 0.0156],\n",
      "         [0.0154, 0.0150, 0.0151,  ..., 0.0155, 0.0160, 0.0155],\n",
      "         [0.0154, 0.0149, 0.0151,  ..., 0.0154, 0.0160, 0.0155],\n",
      "         ...,\n",
      "         [0.0152, 0.0150, 0.0153,  ..., 0.0153, 0.0160, 0.0157],\n",
      "         [0.0152, 0.0151, 0.0155,  ..., 0.0152, 0.0158, 0.0156],\n",
      "         [0.0153, 0.0149, 0.0154,  ..., 0.0151, 0.0156, 0.0153]],\n",
      "\n",
      "        [[0.0151, 0.0150, 0.0150,  ..., 0.0154, 0.0159, 0.0158],\n",
      "         [0.0152, 0.0149, 0.0149,  ..., 0.0156, 0.0157, 0.0159],\n",
      "         [0.0151, 0.0149, 0.0149,  ..., 0.0155, 0.0157, 0.0158],\n",
      "         ...,\n",
      "         [0.0151, 0.0151, 0.0151,  ..., 0.0152, 0.0158, 0.0157],\n",
      "         [0.0151, 0.0154, 0.0152,  ..., 0.0150, 0.0156, 0.0155],\n",
      "         [0.0150, 0.0152, 0.0150,  ..., 0.0150, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0151, 0.0152, 0.0153,  ..., 0.0156, 0.0151, 0.0158],\n",
      "         [0.0152, 0.0152, 0.0153,  ..., 0.0154, 0.0150, 0.0156],\n",
      "         [0.0151, 0.0152, 0.0153,  ..., 0.0155, 0.0151, 0.0154],\n",
      "         ...,\n",
      "         [0.0151, 0.0152, 0.0153,  ..., 0.0154, 0.0150, 0.0158],\n",
      "         [0.0149, 0.0152, 0.0153,  ..., 0.0153, 0.0148, 0.0157],\n",
      "         [0.0150, 0.0153, 0.0154,  ..., 0.0152, 0.0147, 0.0154]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0146, 0.0163,  ..., 0.0161, 0.0159, 0.0154],\n",
      "         [0.0155, 0.0146, 0.0163,  ..., 0.0162, 0.0160, 0.0154],\n",
      "         [0.0155, 0.0146, 0.0163,  ..., 0.0162, 0.0160, 0.0154],\n",
      "         ...,\n",
      "         [0.0154, 0.0146, 0.0163,  ..., 0.0161, 0.0158, 0.0153],\n",
      "         [0.0154, 0.0146, 0.0163,  ..., 0.0161, 0.0158, 0.0153],\n",
      "         [0.0154, 0.0145, 0.0163,  ..., 0.0161, 0.0159, 0.0154]],\n",
      "\n",
      "        [[0.0159, 0.0150, 0.0159,  ..., 0.0166, 0.0163, 0.0151],\n",
      "         [0.0159, 0.0150, 0.0159,  ..., 0.0167, 0.0163, 0.0151],\n",
      "         [0.0159, 0.0150, 0.0159,  ..., 0.0167, 0.0164, 0.0151],\n",
      "         ...,\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0166, 0.0162, 0.0151],\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0166, 0.0162, 0.0151],\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0167, 0.0163, 0.0151]],\n",
      "\n",
      "        [[0.0166, 0.0166, 0.0164,  ..., 0.0167, 0.0154, 0.0153],\n",
      "         [0.0167, 0.0166, 0.0164,  ..., 0.0168, 0.0154, 0.0152],\n",
      "         [0.0167, 0.0166, 0.0164,  ..., 0.0168, 0.0155, 0.0152],\n",
      "         ...,\n",
      "         [0.0166, 0.0166, 0.0164,  ..., 0.0167, 0.0154, 0.0152],\n",
      "         [0.0166, 0.0166, 0.0164,  ..., 0.0167, 0.0154, 0.0153],\n",
      "         [0.0166, 0.0166, 0.0164,  ..., 0.0167, 0.0154, 0.0152]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0160, 0.0155, 0.0150,  ..., 0.0181, 0.0164, 0.0158],\n",
      "         [0.0160, 0.0155, 0.0150,  ..., 0.0181, 0.0165, 0.0158],\n",
      "         [0.0160, 0.0155, 0.0150,  ..., 0.0181, 0.0165, 0.0158],\n",
      "         ...,\n",
      "         [0.0160, 0.0155, 0.0150,  ..., 0.0181, 0.0164, 0.0158],\n",
      "         [0.0160, 0.0155, 0.0150,  ..., 0.0180, 0.0164, 0.0158],\n",
      "         [0.0160, 0.0155, 0.0150,  ..., 0.0182, 0.0165, 0.0158]],\n",
      "\n",
      "        [[0.0159, 0.0153, 0.0153,  ..., 0.0168, 0.0162, 0.0154],\n",
      "         [0.0159, 0.0153, 0.0153,  ..., 0.0169, 0.0164, 0.0154],\n",
      "         [0.0159, 0.0153, 0.0153,  ..., 0.0169, 0.0164, 0.0154],\n",
      "         ...,\n",
      "         [0.0159, 0.0152, 0.0153,  ..., 0.0168, 0.0162, 0.0154],\n",
      "         [0.0158, 0.0152, 0.0153,  ..., 0.0168, 0.0162, 0.0154],\n",
      "         [0.0159, 0.0152, 0.0153,  ..., 0.0168, 0.0163, 0.0154]],\n",
      "\n",
      "        [[0.0156, 0.0158, 0.0154,  ..., 0.0168, 0.0160, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0154,  ..., 0.0169, 0.0161, 0.0158],\n",
      "         [0.0157, 0.0158, 0.0154,  ..., 0.0170, 0.0161, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0158, 0.0154,  ..., 0.0168, 0.0160, 0.0157],\n",
      "         [0.0156, 0.0158, 0.0154,  ..., 0.0168, 0.0160, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0154,  ..., 0.0169, 0.0160, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0149, 0.0150, 0.0156,  ..., 0.0154, 0.0153, 0.0145],\n",
      "         [0.0149, 0.0150, 0.0156,  ..., 0.0154, 0.0152, 0.0145],\n",
      "         [0.0150, 0.0151, 0.0156,  ..., 0.0154, 0.0153, 0.0146],\n",
      "         ...,\n",
      "         [0.0150, 0.0151, 0.0156,  ..., 0.0155, 0.0153, 0.0145],\n",
      "         [0.0150, 0.0151, 0.0156,  ..., 0.0155, 0.0154, 0.0146],\n",
      "         [0.0151, 0.0151, 0.0156,  ..., 0.0155, 0.0154, 0.0146]],\n",
      "\n",
      "        [[0.0150, 0.0149, 0.0146,  ..., 0.0155, 0.0153, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0156, 0.0154, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0145,  ..., 0.0156, 0.0153, 0.0152],\n",
      "         ...,\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0156, 0.0154, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0145,  ..., 0.0157, 0.0155, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0157, 0.0155, 0.0152]],\n",
      "\n",
      "        [[0.0151, 0.0165, 0.0158,  ..., 0.0152, 0.0156, 0.0152],\n",
      "         [0.0151, 0.0165, 0.0158,  ..., 0.0153, 0.0157, 0.0152],\n",
      "         [0.0151, 0.0165, 0.0158,  ..., 0.0153, 0.0157, 0.0152],\n",
      "         ...,\n",
      "         [0.0151, 0.0165, 0.0158,  ..., 0.0153, 0.0157, 0.0152],\n",
      "         [0.0151, 0.0165, 0.0158,  ..., 0.0153, 0.0157, 0.0152],\n",
      "         [0.0151, 0.0165, 0.0157,  ..., 0.0153, 0.0158, 0.0152]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0153, 0.0150, 0.0146,  ..., 0.0172, 0.0156, 0.0152],\n",
      "         [0.0153, 0.0150, 0.0146,  ..., 0.0172, 0.0156, 0.0152],\n",
      "         [0.0154, 0.0150, 0.0146,  ..., 0.0173, 0.0157, 0.0152],\n",
      "         ...,\n",
      "         [0.0154, 0.0150, 0.0146,  ..., 0.0173, 0.0157, 0.0152],\n",
      "         [0.0154, 0.0150, 0.0146,  ..., 0.0173, 0.0157, 0.0152],\n",
      "         [0.0154, 0.0150, 0.0147,  ..., 0.0173, 0.0157, 0.0152]],\n",
      "\n",
      "        [[0.0155, 0.0151, 0.0151,  ..., 0.0158, 0.0153, 0.0151],\n",
      "         [0.0155, 0.0151, 0.0151,  ..., 0.0157, 0.0152, 0.0151],\n",
      "         [0.0155, 0.0151, 0.0151,  ..., 0.0158, 0.0153, 0.0151],\n",
      "         ...,\n",
      "         [0.0156, 0.0151, 0.0151,  ..., 0.0158, 0.0153, 0.0151],\n",
      "         [0.0156, 0.0151, 0.0151,  ..., 0.0159, 0.0154, 0.0152],\n",
      "         [0.0156, 0.0151, 0.0152,  ..., 0.0159, 0.0154, 0.0152]],\n",
      "\n",
      "        [[0.0152, 0.0153, 0.0151,  ..., 0.0151, 0.0156, 0.0148],\n",
      "         [0.0152, 0.0153, 0.0151,  ..., 0.0152, 0.0156, 0.0148],\n",
      "         [0.0152, 0.0153, 0.0152,  ..., 0.0151, 0.0156, 0.0148],\n",
      "         ...,\n",
      "         [0.0153, 0.0153, 0.0151,  ..., 0.0152, 0.0157, 0.0148],\n",
      "         [0.0152, 0.0153, 0.0151,  ..., 0.0152, 0.0157, 0.0148],\n",
      "         [0.0153, 0.0153, 0.0151,  ..., 0.0153, 0.0157, 0.0148]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0151, 0.0162, 0.0157,  ..., 0.0158, 0.0151, 0.0153],\n",
      "         [0.0151, 0.0163, 0.0157,  ..., 0.0159, 0.0151, 0.0153],\n",
      "         [0.0151, 0.0162, 0.0156,  ..., 0.0159, 0.0151, 0.0153],\n",
      "         ...,\n",
      "         [0.0151, 0.0163, 0.0157,  ..., 0.0158, 0.0151, 0.0153],\n",
      "         [0.0151, 0.0162, 0.0157,  ..., 0.0158, 0.0151, 0.0153],\n",
      "         [0.0151, 0.0163, 0.0157,  ..., 0.0158, 0.0151, 0.0153]],\n",
      "\n",
      "        [[0.0153, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0161],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160],\n",
      "         ...,\n",
      "         [0.0153, 0.0162, 0.0159,  ..., 0.0156, 0.0153, 0.0160],\n",
      "         [0.0153, 0.0162, 0.0159,  ..., 0.0156, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160]],\n",
      "\n",
      "        [[0.0155, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0160],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0160],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0159],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0160],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0160],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0152, 0.0159, 0.0160]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0155,  ..., 0.0154, 0.0155, 0.0153],\n",
      "         [0.0158, 0.0158, 0.0155,  ..., 0.0154, 0.0155, 0.0153],\n",
      "         [0.0158, 0.0157, 0.0155,  ..., 0.0154, 0.0155, 0.0153],\n",
      "         ...,\n",
      "         [0.0158, 0.0157, 0.0154,  ..., 0.0154, 0.0155, 0.0153],\n",
      "         [0.0157, 0.0157, 0.0155,  ..., 0.0154, 0.0155, 0.0153],\n",
      "         [0.0158, 0.0158, 0.0155,  ..., 0.0154, 0.0155, 0.0153]],\n",
      "\n",
      "        [[0.0153, 0.0157, 0.0159,  ..., 0.0153, 0.0152, 0.0158],\n",
      "         [0.0153, 0.0158, 0.0159,  ..., 0.0153, 0.0152, 0.0158],\n",
      "         [0.0153, 0.0158, 0.0159,  ..., 0.0153, 0.0152, 0.0158],\n",
      "         ...,\n",
      "         [0.0153, 0.0157, 0.0159,  ..., 0.0153, 0.0152, 0.0158],\n",
      "         [0.0153, 0.0157, 0.0159,  ..., 0.0153, 0.0152, 0.0158],\n",
      "         [0.0153, 0.0158, 0.0159,  ..., 0.0153, 0.0152, 0.0158]],\n",
      "\n",
      "        [[0.0153, 0.0158, 0.0161,  ..., 0.0149, 0.0158, 0.0153],\n",
      "         [0.0154, 0.0158, 0.0161,  ..., 0.0150, 0.0158, 0.0152],\n",
      "         [0.0154, 0.0158, 0.0161,  ..., 0.0150, 0.0158, 0.0152],\n",
      "         ...,\n",
      "         [0.0153, 0.0158, 0.0161,  ..., 0.0150, 0.0158, 0.0153],\n",
      "         [0.0153, 0.0158, 0.0161,  ..., 0.0149, 0.0158, 0.0153],\n",
      "         [0.0154, 0.0158, 0.0161,  ..., 0.0150, 0.0158, 0.0153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0164, 0.0164, 0.0160,  ..., 0.0151, 0.0148, 0.0151],\n",
      "         [0.0164, 0.0164, 0.0160,  ..., 0.0151, 0.0149, 0.0151],\n",
      "         [0.0164, 0.0164, 0.0160,  ..., 0.0151, 0.0148, 0.0151],\n",
      "         ...,\n",
      "         [0.0164, 0.0165, 0.0161,  ..., 0.0150, 0.0147, 0.0151],\n",
      "         [0.0164, 0.0164, 0.0161,  ..., 0.0150, 0.0147, 0.0152],\n",
      "         [0.0164, 0.0164, 0.0161,  ..., 0.0150, 0.0148, 0.0151]],\n",
      "\n",
      "        [[0.0164, 0.0159, 0.0157,  ..., 0.0148, 0.0148, 0.0148],\n",
      "         [0.0164, 0.0159, 0.0156,  ..., 0.0148, 0.0147, 0.0148],\n",
      "         [0.0165, 0.0159, 0.0157,  ..., 0.0147, 0.0147, 0.0148],\n",
      "         ...,\n",
      "         [0.0165, 0.0160, 0.0158,  ..., 0.0146, 0.0146, 0.0147],\n",
      "         [0.0165, 0.0159, 0.0158,  ..., 0.0147, 0.0147, 0.0148],\n",
      "         [0.0165, 0.0159, 0.0157,  ..., 0.0147, 0.0147, 0.0148]],\n",
      "\n",
      "        [[0.0160, 0.0168, 0.0161,  ..., 0.0155, 0.0154, 0.0150],\n",
      "         [0.0160, 0.0168, 0.0160,  ..., 0.0155, 0.0155, 0.0150],\n",
      "         [0.0160, 0.0168, 0.0161,  ..., 0.0155, 0.0155, 0.0150],\n",
      "         ...,\n",
      "         [0.0160, 0.0168, 0.0162,  ..., 0.0155, 0.0154, 0.0150],\n",
      "         [0.0160, 0.0168, 0.0161,  ..., 0.0155, 0.0154, 0.0150],\n",
      "         [0.0160, 0.0168, 0.0161,  ..., 0.0155, 0.0154, 0.0150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0158, 0.0161, 0.0163,  ..., 0.0158, 0.0148, 0.0153],\n",
      "         [0.0158, 0.0161, 0.0162,  ..., 0.0158, 0.0148, 0.0153],\n",
      "         [0.0158, 0.0161, 0.0162,  ..., 0.0158, 0.0148, 0.0153],\n",
      "         ...,\n",
      "         [0.0158, 0.0162, 0.0163,  ..., 0.0158, 0.0148, 0.0153],\n",
      "         [0.0158, 0.0162, 0.0163,  ..., 0.0158, 0.0148, 0.0154],\n",
      "         [0.0158, 0.0161, 0.0163,  ..., 0.0158, 0.0148, 0.0154]],\n",
      "\n",
      "        [[0.0162, 0.0166, 0.0163,  ..., 0.0154, 0.0148, 0.0147],\n",
      "         [0.0162, 0.0166, 0.0162,  ..., 0.0154, 0.0148, 0.0147],\n",
      "         [0.0162, 0.0166, 0.0163,  ..., 0.0153, 0.0148, 0.0147],\n",
      "         ...,\n",
      "         [0.0162, 0.0168, 0.0164,  ..., 0.0152, 0.0147, 0.0146],\n",
      "         [0.0161, 0.0167, 0.0164,  ..., 0.0153, 0.0148, 0.0147],\n",
      "         [0.0162, 0.0167, 0.0164,  ..., 0.0153, 0.0147, 0.0147]],\n",
      "\n",
      "        [[0.0160, 0.0157, 0.0158,  ..., 0.0151, 0.0152, 0.0153],\n",
      "         [0.0160, 0.0157, 0.0158,  ..., 0.0150, 0.0151, 0.0152],\n",
      "         [0.0160, 0.0157, 0.0158,  ..., 0.0150, 0.0151, 0.0152],\n",
      "         ...,\n",
      "         [0.0160, 0.0157, 0.0158,  ..., 0.0150, 0.0150, 0.0152],\n",
      "         [0.0160, 0.0157, 0.0158,  ..., 0.0150, 0.0151, 0.0153],\n",
      "         [0.0160, 0.0157, 0.0158,  ..., 0.0150, 0.0151, 0.0153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0159, 0.0154],\n",
      "         [0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0159, 0.0154],\n",
      "         [0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0159, 0.0155],\n",
      "         ...,\n",
      "         [0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0159, 0.0154],\n",
      "         [0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0159, 0.0154],\n",
      "         [0.0158, 0.0157, 0.0153,  ..., 0.0159, 0.0160, 0.0154]],\n",
      "\n",
      "        [[0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155]],\n",
      "\n",
      "        [[0.0159, 0.0158, 0.0154,  ..., 0.0159, 0.0160, 0.0155],\n",
      "         [0.0159, 0.0157, 0.0154,  ..., 0.0160, 0.0160, 0.0156],\n",
      "         [0.0159, 0.0157, 0.0154,  ..., 0.0160, 0.0160, 0.0156],\n",
      "         ...,\n",
      "         [0.0159, 0.0157, 0.0154,  ..., 0.0160, 0.0160, 0.0155],\n",
      "         [0.0159, 0.0158, 0.0154,  ..., 0.0159, 0.0160, 0.0155],\n",
      "         [0.0159, 0.0158, 0.0154,  ..., 0.0160, 0.0160, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0158, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156],\n",
      "         ...,\n",
      "         [0.0158, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156],\n",
      "         [0.0159, 0.0156, 0.0153,  ..., 0.0161, 0.0160, 0.0156]],\n",
      "\n",
      "        [[0.0161, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154],\n",
      "         [0.0161, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154],\n",
      "         [0.0161, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154],\n",
      "         ...,\n",
      "         [0.0161, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154],\n",
      "         [0.0162, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154],\n",
      "         [0.0162, 0.0156, 0.0155,  ..., 0.0159, 0.0156, 0.0154]],\n",
      "\n",
      "        [[0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155],\n",
      "         [0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155],\n",
      "         [0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155],\n",
      "         ...,\n",
      "         [0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155],\n",
      "         [0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155],\n",
      "         [0.0161, 0.0154, 0.0153,  ..., 0.0157, 0.0159, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0160, 0.0159, 0.0169,  ..., 0.0150, 0.0153, 0.0153],\n",
      "         [0.0160, 0.0160, 0.0169,  ..., 0.0150, 0.0153, 0.0153],\n",
      "         [0.0160, 0.0159, 0.0169,  ..., 0.0150, 0.0153, 0.0153],\n",
      "         ...,\n",
      "         [0.0160, 0.0159, 0.0169,  ..., 0.0150, 0.0152, 0.0153],\n",
      "         [0.0160, 0.0159, 0.0169,  ..., 0.0150, 0.0152, 0.0153],\n",
      "         [0.0160, 0.0159, 0.0169,  ..., 0.0150, 0.0153, 0.0153]],\n",
      "\n",
      "        [[0.0163, 0.0156, 0.0163,  ..., 0.0160, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0156, 0.0163,  ..., 0.0160, 0.0155, 0.0149],\n",
      "         ...,\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149]],\n",
      "\n",
      "        [[0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149],\n",
      "         [0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149],\n",
      "         [0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149],\n",
      "         ...,\n",
      "         [0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149],\n",
      "         [0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149],\n",
      "         [0.0162, 0.0169, 0.0169,  ..., 0.0158, 0.0154, 0.0149]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0162, 0.0171, 0.0158,  ..., 0.0164, 0.0161, 0.0156],\n",
      "         [0.0163, 0.0171, 0.0158,  ..., 0.0165, 0.0161, 0.0156],\n",
      "         [0.0162, 0.0171, 0.0158,  ..., 0.0164, 0.0161, 0.0156],\n",
      "         ...,\n",
      "         [0.0162, 0.0171, 0.0158,  ..., 0.0164, 0.0160, 0.0156],\n",
      "         [0.0162, 0.0171, 0.0158,  ..., 0.0164, 0.0160, 0.0156],\n",
      "         [0.0162, 0.0171, 0.0158,  ..., 0.0164, 0.0161, 0.0156]],\n",
      "\n",
      "        [[0.0162, 0.0168, 0.0171,  ..., 0.0169, 0.0155, 0.0155],\n",
      "         [0.0162, 0.0168, 0.0171,  ..., 0.0169, 0.0155, 0.0155],\n",
      "         [0.0162, 0.0168, 0.0171,  ..., 0.0170, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0162, 0.0168, 0.0171,  ..., 0.0169, 0.0154, 0.0155],\n",
      "         [0.0162, 0.0168, 0.0171,  ..., 0.0169, 0.0154, 0.0155],\n",
      "         [0.0162, 0.0168, 0.0171,  ..., 0.0169, 0.0154, 0.0155]],\n",
      "\n",
      "        [[0.0161, 0.0166, 0.0162,  ..., 0.0161, 0.0151, 0.0157],\n",
      "         [0.0161, 0.0166, 0.0162,  ..., 0.0161, 0.0151, 0.0157],\n",
      "         [0.0161, 0.0166, 0.0162,  ..., 0.0161, 0.0151, 0.0157],\n",
      "         ...,\n",
      "         [0.0161, 0.0166, 0.0162,  ..., 0.0161, 0.0151, 0.0157],\n",
      "         [0.0161, 0.0165, 0.0162,  ..., 0.0161, 0.0151, 0.0157],\n",
      "         [0.0161, 0.0166, 0.0162,  ..., 0.0161, 0.0151, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0149, 0.0157, 0.0159,  ..., 0.0145, 0.0148, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0159,  ..., 0.0145, 0.0148, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0159,  ..., 0.0145, 0.0148, 0.0156],\n",
      "         ...,\n",
      "         [0.0149, 0.0156, 0.0159,  ..., 0.0146, 0.0148, 0.0156],\n",
      "         [0.0149, 0.0156, 0.0159,  ..., 0.0145, 0.0148, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0159,  ..., 0.0145, 0.0148, 0.0156]],\n",
      "\n",
      "        [[0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0149, 0.0152],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0153],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         ...,\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152]],\n",
      "\n",
      "        [[0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0153],\n",
      "         [0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0153],\n",
      "         [0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0153],\n",
      "         ...,\n",
      "         [0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0152],\n",
      "         [0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0152],\n",
      "         [0.0150, 0.0157, 0.0159,  ..., 0.0149, 0.0150, 0.0153]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0152, 0.0151, 0.0155,  ..., 0.0150, 0.0148, 0.0158],\n",
      "         [0.0152, 0.0152, 0.0156,  ..., 0.0150, 0.0148, 0.0158],\n",
      "         [0.0152, 0.0152, 0.0156,  ..., 0.0150, 0.0148, 0.0158],\n",
      "         ...,\n",
      "         [0.0152, 0.0151, 0.0155,  ..., 0.0150, 0.0148, 0.0158],\n",
      "         [0.0152, 0.0151, 0.0155,  ..., 0.0149, 0.0148, 0.0158],\n",
      "         [0.0152, 0.0151, 0.0155,  ..., 0.0149, 0.0148, 0.0158]],\n",
      "\n",
      "        [[0.0151, 0.0154, 0.0152,  ..., 0.0143, 0.0150, 0.0155],\n",
      "         [0.0151, 0.0155, 0.0152,  ..., 0.0143, 0.0150, 0.0155],\n",
      "         [0.0151, 0.0155, 0.0152,  ..., 0.0143, 0.0150, 0.0155],\n",
      "         ...,\n",
      "         [0.0151, 0.0154, 0.0152,  ..., 0.0143, 0.0151, 0.0155],\n",
      "         [0.0150, 0.0154, 0.0152,  ..., 0.0143, 0.0150, 0.0155],\n",
      "         [0.0150, 0.0154, 0.0152,  ..., 0.0143, 0.0150, 0.0155]],\n",
      "\n",
      "        [[0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0147, 0.0157],\n",
      "         [0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0147, 0.0157],\n",
      "         [0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0147, 0.0157],\n",
      "         ...,\n",
      "         [0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0147, 0.0157],\n",
      "         [0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0148, 0.0157],\n",
      "         [0.0150, 0.0154, 0.0157,  ..., 0.0147, 0.0147, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0154, 0.0156, 0.0158,  ..., 0.0156, 0.0153, 0.0153],\n",
      "         [0.0154, 0.0156, 0.0159,  ..., 0.0156, 0.0153, 0.0153],\n",
      "         [0.0154, 0.0156, 0.0159,  ..., 0.0155, 0.0153, 0.0153],\n",
      "         ...,\n",
      "         [0.0154, 0.0156, 0.0159,  ..., 0.0156, 0.0153, 0.0153],\n",
      "         [0.0154, 0.0156, 0.0158,  ..., 0.0156, 0.0153, 0.0154],\n",
      "         [0.0154, 0.0156, 0.0159,  ..., 0.0156, 0.0153, 0.0153]],\n",
      "\n",
      "        [[0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0155, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0151, 0.0153, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0157, 0.0159,  ..., 0.0156, 0.0153, 0.0151],\n",
      "         [0.0155, 0.0157, 0.0159,  ..., 0.0155, 0.0153, 0.0151],\n",
      "         [0.0155, 0.0158, 0.0159,  ..., 0.0156, 0.0153, 0.0151],\n",
      "         ...,\n",
      "         [0.0155, 0.0157, 0.0159,  ..., 0.0155, 0.0153, 0.0151],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0153, 0.0151],\n",
      "         [0.0155, 0.0157, 0.0159,  ..., 0.0156, 0.0153, 0.0151]],\n",
      "\n",
      "        [[0.0151, 0.0156, 0.0158,  ..., 0.0156, 0.0157, 0.0158],\n",
      "         [0.0151, 0.0156, 0.0158,  ..., 0.0156, 0.0157, 0.0158],\n",
      "         [0.0151, 0.0156, 0.0158,  ..., 0.0156, 0.0157, 0.0158],\n",
      "         ...,\n",
      "         [0.0151, 0.0156, 0.0158,  ..., 0.0156, 0.0157, 0.0158],\n",
      "         [0.0151, 0.0156, 0.0158,  ..., 0.0157, 0.0157, 0.0158],\n",
      "         [0.0151, 0.0156, 0.0158,  ..., 0.0157, 0.0157, 0.0158]],\n",
      "\n",
      "        [[0.0151, 0.0160, 0.0160,  ..., 0.0157, 0.0154, 0.0153],\n",
      "         [0.0151, 0.0161, 0.0160,  ..., 0.0157, 0.0154, 0.0153],\n",
      "         [0.0152, 0.0161, 0.0160,  ..., 0.0157, 0.0154, 0.0153],\n",
      "         ...,\n",
      "         [0.0151, 0.0161, 0.0160,  ..., 0.0157, 0.0154, 0.0153],\n",
      "         [0.0152, 0.0161, 0.0160,  ..., 0.0158, 0.0154, 0.0153],\n",
      "         [0.0151, 0.0161, 0.0160,  ..., 0.0158, 0.0154, 0.0153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160],\n",
      "         ...,\n",
      "         [0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0155, 0.0161,  ..., 0.0158, 0.0153, 0.0160]],\n",
      "\n",
      "        [[0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         ...,\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158]],\n",
      "\n",
      "        [[0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159],\n",
      "         ...,\n",
      "         [0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0162, 0.0162,  ..., 0.0160, 0.0152, 0.0159]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0162],\n",
      "         [0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0162],\n",
      "         [0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0161],\n",
      "         ...,\n",
      "         [0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0162],\n",
      "         [0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0162],\n",
      "         [0.0157, 0.0161, 0.0158,  ..., 0.0160, 0.0157, 0.0162]],\n",
      "\n",
      "        [[0.0154, 0.0159, 0.0161,  ..., 0.0159, 0.0156, 0.0158],\n",
      "         [0.0154, 0.0158, 0.0161,  ..., 0.0159, 0.0156, 0.0158],\n",
      "         [0.0154, 0.0159, 0.0161,  ..., 0.0159, 0.0156, 0.0158],\n",
      "         ...,\n",
      "         [0.0154, 0.0159, 0.0162,  ..., 0.0159, 0.0156, 0.0158],\n",
      "         [0.0154, 0.0158, 0.0162,  ..., 0.0159, 0.0156, 0.0158],\n",
      "         [0.0154, 0.0158, 0.0162,  ..., 0.0159, 0.0156, 0.0158]],\n",
      "\n",
      "        [[0.0154, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161],\n",
      "         [0.0153, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161],\n",
      "         [0.0153, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161],\n",
      "         ...,\n",
      "         [0.0154, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161],\n",
      "         [0.0154, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161],\n",
      "         [0.0154, 0.0161, 0.0160,  ..., 0.0157, 0.0158, 0.0161]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0158],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0158],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0158],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0155, 0.0153, 0.0158]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0154, 0.0157],\n",
      "         [0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0153, 0.0157],\n",
      "         [0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0153, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0153, 0.0157],\n",
      "         [0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0153, 0.0157],\n",
      "         [0.0155, 0.0158, 0.0158,  ..., 0.0156, 0.0153, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158],\n",
      "         ...,\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158],\n",
      "         [0.0155, 0.0157, 0.0158,  ..., 0.0155, 0.0154, 0.0158]],\n",
      "\n",
      "        [[0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157],\n",
      "         [0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157],\n",
      "         [0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157],\n",
      "         ...,\n",
      "         [0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157],\n",
      "         [0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157],\n",
      "         [0.0154, 0.0157, 0.0158,  ..., 0.0155, 0.0155, 0.0157]],\n",
      "\n",
      "        [[0.0154, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158],\n",
      "         [0.0154, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158],\n",
      "         [0.0153, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158],\n",
      "         ...,\n",
      "         [0.0154, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158],\n",
      "         [0.0154, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158],\n",
      "         [0.0153, 0.0158, 0.0158,  ..., 0.0155, 0.0155, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0154, 0.0153,  ..., 0.0153, 0.0152, 0.0152],\n",
      "         [0.0153, 0.0154, 0.0153,  ..., 0.0153, 0.0152, 0.0152],\n",
      "         [0.0153, 0.0154, 0.0153,  ..., 0.0153, 0.0152, 0.0152],\n",
      "         ...,\n",
      "         [0.0153, 0.0154, 0.0153,  ..., 0.0153, 0.0152, 0.0152],\n",
      "         [0.0153, 0.0154, 0.0153,  ..., 0.0153, 0.0151, 0.0152],\n",
      "         [0.0153, 0.0153, 0.0153,  ..., 0.0153, 0.0151, 0.0152]],\n",
      "\n",
      "        [[0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0153],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154]],\n",
      "\n",
      "        [[0.0154, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155],\n",
      "         [0.0154, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155],\n",
      "         [0.0154, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155],\n",
      "         ...,\n",
      "         [0.0153, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155],\n",
      "         [0.0154, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155],\n",
      "         [0.0154, 0.0156, 0.0154,  ..., 0.0152, 0.0151, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153],\n",
      "         [0.0155, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153],\n",
      "         [0.0155, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153],\n",
      "         ...,\n",
      "         [0.0155, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153],\n",
      "         [0.0154, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153],\n",
      "         [0.0154, 0.0151, 0.0153,  ..., 0.0152, 0.0150, 0.0153]],\n",
      "\n",
      "        [[0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151],\n",
      "         [0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151],\n",
      "         [0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151],\n",
      "         [0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151],\n",
      "         [0.0155, 0.0155, 0.0151,  ..., 0.0148, 0.0149, 0.0151]],\n",
      "\n",
      "        [[0.0155, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152],\n",
      "         [0.0155, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152],\n",
      "         [0.0154, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152],\n",
      "         ...,\n",
      "         [0.0154, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152],\n",
      "         [0.0154, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152],\n",
      "         [0.0154, 0.0153, 0.0154,  ..., 0.0150, 0.0154, 0.0152]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0160, 0.0163, 0.0162,  ..., 0.0153, 0.0151, 0.0159],\n",
      "         [0.0161, 0.0163, 0.0162,  ..., 0.0153, 0.0150, 0.0159],\n",
      "         [0.0161, 0.0163, 0.0162,  ..., 0.0153, 0.0150, 0.0159],\n",
      "         ...,\n",
      "         [0.0161, 0.0163, 0.0162,  ..., 0.0153, 0.0151, 0.0159],\n",
      "         [0.0161, 0.0163, 0.0162,  ..., 0.0153, 0.0151, 0.0159],\n",
      "         [0.0161, 0.0163, 0.0162,  ..., 0.0153, 0.0150, 0.0159]],\n",
      "\n",
      "        [[0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         ...,\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155]],\n",
      "\n",
      "        [[0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156],\n",
      "         [0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156],\n",
      "         [0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156],\n",
      "         ...,\n",
      "         [0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156],\n",
      "         [0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156],\n",
      "         [0.0159, 0.0156, 0.0162,  ..., 0.0156, 0.0154, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0159, 0.0163, 0.0166,  ..., 0.0148, 0.0153, 0.0159],\n",
      "         [0.0159, 0.0163, 0.0166,  ..., 0.0148, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0163, 0.0166,  ..., 0.0148, 0.0152, 0.0159],\n",
      "         ...,\n",
      "         [0.0159, 0.0162, 0.0166,  ..., 0.0148, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0162, 0.0166,  ..., 0.0148, 0.0152, 0.0159],\n",
      "         [0.0159, 0.0163, 0.0166,  ..., 0.0148, 0.0152, 0.0159]],\n",
      "\n",
      "        [[0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158],\n",
      "         [0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158],\n",
      "         [0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158],\n",
      "         [0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158],\n",
      "         [0.0156, 0.0164, 0.0164,  ..., 0.0154, 0.0153, 0.0158]],\n",
      "\n",
      "        [[0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159],\n",
      "         [0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159],\n",
      "         [0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159],\n",
      "         ...,\n",
      "         [0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159],\n",
      "         [0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159],\n",
      "         [0.0156, 0.0160, 0.0164,  ..., 0.0153, 0.0153, 0.0159]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0158, 0.0156,  ..., 0.0157, 0.0157, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156]],\n",
      "\n",
      "        [[0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         ...,\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0155]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153],\n",
      "         [0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153],\n",
      "         [0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153],\n",
      "         ...,\n",
      "         [0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153],\n",
      "         [0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153],\n",
      "         [0.0157, 0.0153, 0.0151,  ..., 0.0158, 0.0160, 0.0153]],\n",
      "\n",
      "        [[0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155]],\n",
      "\n",
      "        [[0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155],\n",
      "         [0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155],\n",
      "         [0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155],\n",
      "         [0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155],\n",
      "         [0.0155, 0.0153, 0.0151,  ..., 0.0155, 0.0158, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152],\n",
      "         [0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152],\n",
      "         [0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152],\n",
      "         ...,\n",
      "         [0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152],\n",
      "         [0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152],\n",
      "         [0.0155, 0.0150, 0.0152,  ..., 0.0157, 0.0157, 0.0152]],\n",
      "\n",
      "        [[0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154],\n",
      "         [0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154],\n",
      "         [0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154],\n",
      "         ...,\n",
      "         [0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154],\n",
      "         [0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154],\n",
      "         [0.0158, 0.0152, 0.0150,  ..., 0.0155, 0.0157, 0.0154]],\n",
      "\n",
      "        [[0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152],\n",
      "         [0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152],\n",
      "         [0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152],\n",
      "         ...,\n",
      "         [0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152],\n",
      "         [0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152],\n",
      "         [0.0157, 0.0152, 0.0151,  ..., 0.0156, 0.0158, 0.0152]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156]],\n",
      "\n",
      "        [[0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156],\n",
      "         ...,\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0156]],\n",
      "\n",
      "        [[0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         ...,\n",
      "         [0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156],\n",
      "         [0.0158, 0.0156, 0.0155,  ..., 0.0156, 0.0158, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         ...,\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158]],\n",
      "\n",
      "        [[0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157],\n",
      "         [0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157],\n",
      "         [0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157],\n",
      "         [0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157],\n",
      "         [0.0153, 0.0153, 0.0154,  ..., 0.0155, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0157,  ..., 0.0154, 0.0157, 0.0156]],\n",
      "\n",
      "        [[0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0160],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0160],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0160],\n",
      "         ...,\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0160],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0159],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0156, 0.0157, 0.0160]],\n",
      "\n",
      "        [[0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0158, 0.0155, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0158]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0158, 0.0157, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0154, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0154, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0154, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0154, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0158, 0.0157,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0157, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0158,  ..., 0.0155, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0154, 0.0154, 0.0155]],\n",
      "\n",
      "        [[0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155]],\n",
      "\n",
      "        [[0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155],\n",
      "         ...,\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155],\n",
      "         [0.0154, 0.0155, 0.0157,  ..., 0.0155, 0.0154, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156],\n",
      "         [0.0154, 0.0155, 0.0155,  ..., 0.0154, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0155, 0.0155, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0158, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0158, 0.0159, 0.0159]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158]],\n",
      "\n",
      "        [[0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0159, 0.0159, 0.0158]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0157, 0.0158, 0.0159]],\n",
      "\n",
      "        [[0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         ...,\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0159],\n",
      "         [0.0157, 0.0155, 0.0156,  ..., 0.0158, 0.0159, 0.0158]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0159]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0155]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0155,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0155, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0155, 0.0156,  ..., 0.0157, 0.0158, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0157, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0155, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0155, 0.0155, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0156, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0157, 0.0156, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0158, 0.0158, 0.0158]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158]],\n",
      "\n",
      "        [[0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0155, 0.0154, 0.0154,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0155, 0.0156, 0.0157]],\n",
      "\n",
      "        [[0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159],\n",
      "         ...,\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159],\n",
      "         [0.0155, 0.0155, 0.0156,  ..., 0.0157, 0.0159, 0.0159]],\n",
      "\n",
      "        [[0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0155, 0.0157,  ..., 0.0158, 0.0157, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155]],\n",
      "\n",
      "        [[0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0157, 0.0155]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155],\n",
      "         [0.0157, 0.0155, 0.0154,  ..., 0.0157, 0.0157, 0.0155]],\n",
      "\n",
      "        [[0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0155, 0.0155,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0156, 0.0156, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:116: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i] = torch.sin(torch.range(start=0, end=self.d_model-1, step=1)/(10000 ** (2*i/self.input_len)))\n",
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:117: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i+1] = torch.cos(torch.range(start=1, end=self.d_model, step=1)/(10000 ** (2*i/self.input_len)))\n",
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = F.softmax(torch.matmul(q, torch.permute(k, (0, 2, 1)))/torch.sqrt(torch.tensor(self.d_k)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[325], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[319], line 21\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x_input, x_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m enc_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_input)\n\u001b[1;32m     19\u001b[0m _, dec_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder((enc_embed, x_output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder()))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(dec_embed)\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, target)\n",
    "\n",
    "        loss = loss_fn(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3821e-01, 6.1754e-01, 8.0191e-02,  ..., 6.2129e-05,\n",
      "          1.7153e-03, 2.4272e-02],\n",
      "         [1.3089e-01, 6.3429e-01, 1.0406e-01,  ..., 2.5829e-05,\n",
      "          7.3846e-04, 1.2821e-02],\n",
      "         [1.3619e-01, 6.1736e-01, 1.0914e-01,  ..., 3.4328e-05,\n",
      "          7.3727e-04, 1.2684e-02],\n",
      "         ...,\n",
      "         [1.3129e-01, 5.6432e-01, 9.0591e-02,  ..., 1.3236e-04,\n",
      "          5.0031e-03, 3.8740e-02],\n",
      "         [1.3128e-01, 5.1454e-01, 8.4174e-02,  ..., 2.2848e-04,\n",
      "          8.8855e-03, 5.4875e-02],\n",
      "         [1.2780e-01, 6.0766e-01, 9.2235e-02,  ..., 5.9346e-05,\n",
      "          2.7370e-03, 2.7681e-02]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[2.2040e-03, 7.7175e-01, 1.7789e-01,  ..., 2.6971e-09,\n",
      "          1.2282e-04, 1.7028e-03],\n",
      "         [2.7495e-03, 7.7112e-01, 1.6558e-01,  ..., 4.2838e-09,\n",
      "          1.7010e-04, 2.6730e-03],\n",
      "         [3.6068e-03, 7.3419e-01, 1.6952e-01,  ..., 1.6057e-08,\n",
      "          2.9806e-04, 4.5703e-03],\n",
      "         ...,\n",
      "         [5.3590e-03, 7.3242e-01, 1.8988e-01,  ..., 3.1779e-08,\n",
      "          4.1307e-04, 4.2546e-03],\n",
      "         [2.7435e-03, 7.3087e-01, 1.8140e-01,  ..., 1.1414e-08,\n",
      "          2.1989e-04, 3.1473e-03],\n",
      "         [2.8678e-03, 7.7358e-01, 1.5876e-01,  ..., 4.2962e-09,\n",
      "          1.7799e-04, 2.9287e-03]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.0691e-04, 5.6947e-02, 2.4888e-03,  ..., 9.0331e-04,\n",
      "          7.7224e-03, 4.3012e-01],\n",
      "         [1.4774e-04, 6.8601e-02, 2.9108e-03,  ..., 2.1282e-04,\n",
      "          4.8397e-03, 3.7887e-01],\n",
      "         [1.6447e-04, 6.0945e-02, 3.3231e-03,  ..., 3.7708e-04,\n",
      "          7.6633e-03, 4.2258e-01],\n",
      "         ...,\n",
      "         [3.5856e-04, 6.8283e-02, 2.4249e-03,  ..., 6.1700e-04,\n",
      "          5.2869e-03, 3.7705e-01],\n",
      "         [2.0355e-04, 6.7224e-02, 2.0279e-03,  ..., 3.2807e-04,\n",
      "          4.3445e-03, 3.9829e-01],\n",
      "         [1.4193e-04, 7.7547e-02, 2.2679e-03,  ..., 1.2741e-04,\n",
      "          2.9148e-03, 3.4212e-01]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0340, 0.0454, 0.0017,  ..., 0.0105, 0.0252, 0.0576],\n",
      "         [0.0292, 0.0522, 0.0011,  ..., 0.0065, 0.0159, 0.0493],\n",
      "         [0.0317, 0.0586, 0.0014,  ..., 0.0079, 0.0179, 0.0535],\n",
      "         ...,\n",
      "         [0.0330, 0.0302, 0.0013,  ..., 0.0139, 0.0304, 0.0621],\n",
      "         [0.0276, 0.0188, 0.0012,  ..., 0.0149, 0.0420, 0.0553],\n",
      "         [0.0300, 0.0306, 0.0009,  ..., 0.0090, 0.0221, 0.0554]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0119, 0.0116, 0.0116,  ..., 0.0151, 0.0169, 0.0151],\n",
      "         [0.0121, 0.0118, 0.0117,  ..., 0.0151, 0.0168, 0.0152],\n",
      "         [0.0122, 0.0119, 0.0118,  ..., 0.0150, 0.0169, 0.0152],\n",
      "         ...,\n",
      "         [0.0119, 0.0116, 0.0115,  ..., 0.0151, 0.0168, 0.0151],\n",
      "         [0.0118, 0.0116, 0.0116,  ..., 0.0151, 0.0169, 0.0152],\n",
      "         [0.0120, 0.0117, 0.0116,  ..., 0.0152, 0.0167, 0.0151]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0158, 0.0164, 0.0147,  ..., 0.0134, 0.0141, 0.0154],\n",
      "         [0.0158, 0.0166, 0.0148,  ..., 0.0132, 0.0140, 0.0153],\n",
      "         [0.0159, 0.0167, 0.0147,  ..., 0.0131, 0.0139, 0.0154],\n",
      "         ...,\n",
      "         [0.0157, 0.0165, 0.0149,  ..., 0.0135, 0.0141, 0.0154],\n",
      "         [0.0157, 0.0165, 0.0149,  ..., 0.0136, 0.0141, 0.0154],\n",
      "         [0.0157, 0.0165, 0.0150,  ..., 0.0135, 0.0141, 0.0153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0173, 0.0179,  ..., 0.0155, 0.0175, 0.0160],\n",
      "         [0.0152, 0.0172, 0.0178,  ..., 0.0156, 0.0173, 0.0160],\n",
      "         [0.0152, 0.0171, 0.0178,  ..., 0.0156, 0.0173, 0.0160],\n",
      "         ...,\n",
      "         [0.0155, 0.0174, 0.0176,  ..., 0.0155, 0.0176, 0.0161],\n",
      "         [0.0155, 0.0174, 0.0177,  ..., 0.0154, 0.0175, 0.0161],\n",
      "         [0.0154, 0.0172, 0.0176,  ..., 0.0155, 0.0175, 0.0161]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0150, 0.0148, 0.0149,  ..., 0.0158, 0.0157, 0.0152],\n",
      "         [0.0150, 0.0147, 0.0149,  ..., 0.0157, 0.0157, 0.0151],\n",
      "         [0.0151, 0.0149, 0.0150,  ..., 0.0157, 0.0156, 0.0152],\n",
      "         ...,\n",
      "         [0.0151, 0.0149, 0.0152,  ..., 0.0154, 0.0156, 0.0151],\n",
      "         [0.0152, 0.0151, 0.0154,  ..., 0.0152, 0.0155, 0.0150],\n",
      "         [0.0150, 0.0148, 0.0152,  ..., 0.0152, 0.0155, 0.0149]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0159, 0.0150, 0.0159,  ..., 0.0166, 0.0163, 0.0151],\n",
      "         [0.0159, 0.0150, 0.0159,  ..., 0.0167, 0.0163, 0.0151],\n",
      "         [0.0159, 0.0150, 0.0159,  ..., 0.0167, 0.0164, 0.0151],\n",
      "         ...,\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0166, 0.0162, 0.0151],\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0166, 0.0162, 0.0151],\n",
      "         [0.0158, 0.0150, 0.0159,  ..., 0.0167, 0.0163, 0.0151]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0150, 0.0149, 0.0146,  ..., 0.0155, 0.0153, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0156, 0.0154, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0145,  ..., 0.0156, 0.0153, 0.0152],\n",
      "         ...,\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0156, 0.0154, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0145,  ..., 0.0157, 0.0155, 0.0152],\n",
      "         [0.0150, 0.0149, 0.0146,  ..., 0.0157, 0.0155, 0.0152]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0161],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160],\n",
      "         ...,\n",
      "         [0.0153, 0.0162, 0.0159,  ..., 0.0156, 0.0153, 0.0160],\n",
      "         [0.0153, 0.0162, 0.0159,  ..., 0.0156, 0.0153, 0.0160],\n",
      "         [0.0154, 0.0162, 0.0160,  ..., 0.0157, 0.0153, 0.0160]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0164, 0.0159, 0.0157,  ..., 0.0148, 0.0148, 0.0148],\n",
      "         [0.0164, 0.0159, 0.0156,  ..., 0.0148, 0.0147, 0.0148],\n",
      "         [0.0165, 0.0159, 0.0157,  ..., 0.0147, 0.0147, 0.0148],\n",
      "         ...,\n",
      "         [0.0165, 0.0160, 0.0158,  ..., 0.0146, 0.0146, 0.0147],\n",
      "         [0.0165, 0.0159, 0.0158,  ..., 0.0147, 0.0147, 0.0148],\n",
      "         [0.0165, 0.0159, 0.0157,  ..., 0.0147, 0.0147, 0.0148]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         ...,\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155],\n",
      "         [0.0160, 0.0155, 0.0153,  ..., 0.0157, 0.0156, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0163, 0.0156, 0.0163,  ..., 0.0160, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0156, 0.0163,  ..., 0.0160, 0.0155, 0.0149],\n",
      "         ...,\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149],\n",
      "         [0.0163, 0.0157, 0.0163,  ..., 0.0159, 0.0155, 0.0149]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0149, 0.0152],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0153],\n",
      "         [0.0153, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         ...,\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152],\n",
      "         [0.0152, 0.0154, 0.0156,  ..., 0.0148, 0.0150, 0.0152]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0149, 0.0157, 0.0161,  ..., 0.0157, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         ...,\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158],\n",
      "         [0.0159, 0.0160, 0.0162,  ..., 0.0157, 0.0156, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0153],\n",
      "         [0.0155, 0.0156, 0.0152,  ..., 0.0150, 0.0149, 0.0154]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0156],\n",
      "         ...,\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155],\n",
      "         [0.0161, 0.0161, 0.0164,  ..., 0.0155, 0.0153, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         ...,\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155],\n",
      "         [0.0154, 0.0153, 0.0150,  ..., 0.0156, 0.0157, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0155,  ..., 0.0157, 0.0158, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         ...,\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158],\n",
      "         [0.0153, 0.0155, 0.0155,  ..., 0.0158, 0.0158, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0155, 0.0156, 0.0157,  ..., 0.0158, 0.0158, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0154, 0.0155, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0155, 0.0155, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
      "         [0.0155, 0.0156, 0.0156,  ..., 0.0155, 0.0155, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0157, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0155, 0.0155]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0158, 0.0158, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0157, 0.0157, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0157, 0.0156, 0.0157]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0157,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         ...,\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156],\n",
      "         [0.0157, 0.0156, 0.0156,  ..., 0.0156, 0.0157, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         ...,\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158],\n",
      "         [0.0156, 0.0156, 0.0156,  ..., 0.0158, 0.0159, 0.0158]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         ...,\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156],\n",
      "         [0.0156, 0.0155, 0.0155,  ..., 0.0157, 0.0156, 0.0156]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:116: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i] = torch.sin(torch.range(start=0, end=self.d_model-1, step=1)/(10000 ** (2*i/self.input_len)))\n",
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:117: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  pos[:, 2*i+1] = torch.cos(torch.range(start=1, end=self.d_model, step=1)/(10000 ** (2*i/self.input_len)))\n",
      "/var/folders/j8/f15cwy8d2y997yr276w5nd6m0000gn/T/ipykernel_57592/1442265889.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = F.softmax(torch.matmul(q, torch.permute(k, (0, 2, 1)))/torch.sqrt(torch.tensor(self.d_k)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[327], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[319], line 21\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x_input, x_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m enc_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_input)\n\u001b[1;32m     19\u001b[0m _, dec_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder((enc_embed, x_output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder()))\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(dec_embed)\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = model(data[1:2], target[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (32, 64, 16) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[328], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(torch\u001b[38;5;241m.\u001b[39msqueeze(output)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/pyplot.py:3358\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3339\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3358\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3369\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3373\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3377\u001b[0m     sci(__ret)\n\u001b[1;32m   3378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5759\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5759\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5760\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5762\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/image.py:693\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    691\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (32, 64, 16) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(torch.squeeze(torch.softmax(output, -1)).detach().numpy())\n",
    "plt.imshow(torch.squeeze(output).detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(torch.squeeze(target[0]).detach().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
