# Schlag et al. 2021 Linear Transformers are Secretly Fast Weight Programmers

### Overview
This repo contains an unoffical Python implementation of some of the results from the paper [*Linear Transformers are Secretly Fast Weight Programmers*](https://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf) by Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. 

### Results
<p align="center">
<img src="https://github.com/et22/paper-implementations/blob/main/schlag2021_linear_transformers/figure2.png" alt="Loss of different attention mechanisms." width="900"/>
</p>

*Figure 1.* Loss of softmax attention and various linear attention mechanisms on a toy associative retrieval task.

To reproduce, run ```sh run_all.sh```.
### References
Referenced the original paper and the associated [code](https://github.com/ischlag/fast-weight-transformers/tree/main).
